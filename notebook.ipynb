{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vicuna-7b-GPTQ-4bit-128g...\n",
      "Loading vicuna-7b-GPTQ-4bit-128g...\n",
      "Auto-assiging --gpu-memory 23 for your GPU to try to prevent out-of-memory errors. You can manually set other values.\n",
      "The AutoGPTQ params are: {'model_basename': 'vicuna-7b-v1.1-4bit-128g', 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp': False, 'use_safetensors': True, 'trust_remote_code': True, 'max_memory': {0: '23GiB', 'cpu': '32GiB'}, 'quantize_config': BaseQuantizeConfig(bits=4, group_size=128, damp_percent=0.01, desc_act=True, sym=True, true_sequential=True, model_name_or_path=None, model_file_base_name=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at models/vicuna-7b-GPTQ-4bit-128g/vicuna-7b-v1.1-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the universal LLaMA tokenizer from models/oobabooga_llama-tokenizer...\n",
      "Loaded the model in 8.08 seconds.\n",
      "\n",
      "Loaded the model in 8.08 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (AutoConfig, AutoModelForCausalLM, AutoTokenizer,\n",
    "                          BitsAndBytesConfig)\n",
    "\n",
    "\n",
    "from modules.GPTQ_loader import load_quantized\n",
    "from modules.text_generation import generate_reply, plot_char_freq\n",
    "import modules.shared as shared\n",
    "from modules.model import load_model\n",
    "\n",
    "#shared.model_name = \"alpaca-native-4bit\"\n",
    "#shared.model_name = \"LLaMA-30B-4bit-128g\"\n",
    "#shared.model_name = \"vicuna-13b-GPTQ-4bit-128g\"\n",
    "shared.model_name = \"vicuna-7b-GPTQ-4bit-128g\"\n",
    "#shared.model_name = \"Wizard-Vicuna-13B-4bit-128g\"\n",
    "#shared.model_name = \"LLaMA-7B-HF\"\n",
    "\n",
    "print(f\"Loading {shared.model_name}...\")\n",
    "t0 = time.time()\n",
    "#shared.model = load_quantized(shared.model_name)\n",
    "#shared.tokenizer = AutoTokenizer.from_pretrained(Path(f\"models/{shared.model_name}/\"))\n",
    "#shared.tokenizer.truncation_side = 'left'\n",
    "\n",
    "shared.groupsize = 128\n",
    "shared.wbits = 4\n",
    "shared.model, shared.tokenizer = load_model(shared.model_name, gptq = True)\n",
    "\n",
    "print(f\"Loaded the model in {(time.time()-t0):.2f} seconds.\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Lancaster_sensorimotor_norms_for_39707_words.csv', header=0)  #the header is in the first row\n",
    "\n",
    "words = df.iloc[:, 0]\n",
    "value = df.iloc[:, 34]\n",
    "\n",
    "selected_columns = df[['Word', 'Dominant.sensorimotor']]  # Specify the columns you want to convert to a dictionary\n",
    "shared.sensorimotor = selected_columns.set_index('Word')['Dominant.sensorimotor'].to_dict()\n",
    "#classes: ['Interoceptive', 'Olfactory', 'Mouth', 'Torso', 'Gustatory', 'Visual', 'Foot_leg', 'Auditory', 'Hand_arm', 'Haptic', 'Head']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def most_common_letters(text, num_letters=3):\n",
    "    sentences = re.split(r'[.!?]+', text.lower())  # Split the text into sentences\n",
    "    common_letters = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        letters = re.findall(r'\\w', sentence)  # Find all alphanumeric characters in the sentence\n",
    "        letter_counts = Counter(letters)  # Count the occurrence of each letter\n",
    "        \n",
    "        # Find the most common letters\n",
    "        most_common = [letter for letter, count in letter_counts.most_common(num_letters)]\n",
    "        \n",
    "        common_letters.append(most_common)\n",
    "    \n",
    "    return common_letters\n",
    "\n",
    "def first_letters(text):\n",
    "    sentences = re.split(r'[.!?]+', text)  # Split the text into sentences\n",
    "    first_letters = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        first_letter = re.search(r'\\w', sentence)  # Find the first alphanumeric character in the sentence\n",
    "        if first_letter:\n",
    "            first_letters.append(first_letter.group())\n",
    "    \n",
    "    return first_letters\n",
    "\n",
    "\n",
    "#table = pd.read_csv('Lancaster_sensorimotor_norms_for_39707_words.csv')\n",
    "#table = pd.read_csv('your_file.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Human: Explain in detail: What is the 24 Hours of Le Mans? ### Assistant:\n",
      "--------------------\n",
      "Output generated in 22.03 seconds (13.85 tokens/s, 305 tokens, context 24, seed 0)\n",
      "  The 24 Hours of Le Mans is a endurance race that takes place at the Circuit de la Sarthe, near the town of Le Mans, France. Until recently it was\n",
      "one of the most prestigious and challenging sports car races in the world. GT cars (Grand Tourer) such as Porsche, Ferrari, Audi, Ford, BMW, Jaguar,\n",
      "Nissan, etc compete against each other for 24 hours straight. Race starts on Saturday evening and ends Sunday afternoon. The track is long and narrow\n",
      "with high-speed corners which require precise driving skills to navigate safely. Zero tolerance policy towards any kind of cheating or misbehavior\n",
      "makes this event even more exciting. Teams have to work together perfectly to finish the race successfully. Until recent years, it has been dominated\n",
      "by European manufacturers like Porsche, Ferrari, and Audi but now American brands are also making their presence felt. GTE class is open to Grand\n",
      "Tourers and Touring Cars, while LMP1 class is dedicated to prototype racing cars. Recently FIA WEC (World Endurance Championship) replaced the old Le\n",
      "Mans series and it's considered as the highest level of sportscar racing after Formula One. It's not only about winning the race but also about\n",
      "finishing it within the time limit and maintaining the safety standards set by the organizers. Zero fatality rule applies here too!</s>\n"
     ]
    }
   ],
   "source": [
    "question = \"### Human: Explain in detail: What is the 24 Hours of Le Mans? ### Assistant:\"\n",
    "\n",
    "\n",
    "generate_params = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'add_bos_token' : False,\n",
    "    'truncation_length' : 2048,\n",
    "    'custom_stopping_strings' : [\"### Human:\", \"Human:\", \"user:\"],\n",
    "    'ban_eos_token' : False,\n",
    "    'skip_special_tokens' : False,\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.1,\n",
    "    'typical_p': 1,\n",
    "    'repetition_penalty': 1.176,\n",
    "    'encoder_repetition_penalty': 1,\n",
    "    'top_k': 40,\n",
    "    'num_beams': 1,\n",
    "    'penalty_alpha': 0,\n",
    "    'min_length': 0,\n",
    "    'length_penalty': 1,\n",
    "    'no_repeat_ngram_size': 0,\n",
    "    'early_stopping': False,\n",
    "    'seed' : 0,\n",
    "}\n",
    "shared.delta = 4.00\n",
    "shared.delta_first = 5000.0\n",
    "shared.code = \"tugraz\"\n",
    "reply = generate_reply(question, generate_params)\n",
    "\n",
    "wrapped_string = \"\\n\".join(textwrap.wrap(reply, width=150))\n",
    "print(wrapped_string)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "24\n",
      "Hours\n",
      "of\n",
      "Le\n",
      "Mans\n",
      "is\n",
      "a\n",
      "endurance\n",
      "race\n",
      "that\n",
      "takes\n",
      "place\n",
      "at\n",
      "the\n",
      "Circuit\n",
      "de\n",
      "la\n",
      "Sarthe,\n",
      "near\n",
      "the\n",
      "town\n",
      "of\n",
      "Le\n",
      "Mans,\n",
      "France.\n",
      "Until\n",
      "recently\n",
      "it\n",
      "was\n",
      "one\n",
      "of\n",
      "the\n",
      "most\n",
      "prestigious\n",
      "and\n",
      "challenging\n",
      "sports\n",
      "car\n",
      "races\n",
      "in\n",
      "the\n",
      "world.\n",
      "GT\n",
      "cars\n",
      "(Grand\n",
      "Tourer)\n",
      "such\n",
      "as\n",
      "Porsche,\n",
      "Ferrari,\n",
      "Audi,\n",
      "Ford,\n",
      "BMW,\n",
      "Jaguar,\n",
      "Nissan,\n",
      "etc\n",
      "compete\n",
      "against\n",
      "each\n",
      "other\n",
      "for\n",
      "24\n",
      "hours\n",
      "straight.\n",
      "Race\n",
      "starts\n",
      "on\n",
      "Saturday\n",
      "evening\n",
      "and\n",
      "ends\n",
      "Sunday\n",
      "afternoon.\n",
      "The\n",
      "track\n",
      "is\n",
      "long\n",
      "and\n",
      "narrow\n",
      "with\n",
      "high-speed\n",
      "corners\n",
      "which\n",
      "require\n",
      "precise\n",
      "driving\n",
      "skills\n",
      "to\n",
      "navigate\n",
      "safely.\n",
      "Zero\n",
      "tolerance\n",
      "policy\n",
      "towards\n",
      "any\n",
      "kind\n",
      "of\n",
      "cheating\n",
      "or\n",
      "misbehavior\n",
      "makes\n",
      "this\n",
      "event\n",
      "even\n",
      "more\n",
      "exciting.\n",
      "Teams\n",
      "have\n",
      "to\n",
      "work\n",
      "together\n",
      "perfectly\n",
      "to\n",
      "finish\n",
      "the\n",
      "race\n",
      "successfully.\n",
      "Until\n",
      "recent\n",
      "years,\n",
      "it\n",
      "has\n",
      "been\n",
      "dominated\n",
      "by\n",
      "European\n",
      "manufacturers\n",
      "like\n",
      "Porsche,\n",
      "Ferrari,\n",
      "and\n",
      "Audi\n",
      "but\n",
      "now\n",
      "American\n",
      "brands\n",
      "are\n",
      "also\n",
      "making\n",
      "their\n",
      "presence\n",
      "felt.\n",
      "GTE\n",
      "class\n",
      "is\n",
      "open\n",
      "to\n",
      "Grand\n",
      "Tourers\n",
      "and\n",
      "Touring\n",
      "Cars,\n",
      "while\n",
      "LMP1\n",
      "class\n",
      "is\n",
      "dedicated\n",
      "to\n",
      "prototype\n",
      "racing\n",
      "cars.\n",
      "Recently\n",
      "FIA\n",
      "WEC\n",
      "(World\n",
      "Endurance\n",
      "Championship)\n",
      "replaced\n",
      "the\n",
      "old\n",
      "Le\n",
      "Mans\n",
      "series\n",
      "and\n",
      "it's\n",
      "considered\n",
      "as\n",
      "the\n",
      "highest\n",
      "level\n",
      "of\n",
      "sportscar\n",
      "racing\n",
      "after\n",
      "Formula\n",
      "One.\n",
      "It's\n",
      "not\n",
      "only\n",
      "about\n",
      "winning\n",
      "the\n",
      "race\n",
      "but\n",
      "also\n",
      "about\n",
      "finishing\n",
      "it\n",
      "within\n",
      "the\n",
      "time\n",
      "limit\n",
      "and\n",
      "maintaining\n",
      "the\n",
      "safety\n",
      "standards\n",
      "set\n",
      "by\n",
      "the\n",
      "organizers.\n",
      "Zero\n",
      "fatality\n",
      "rule\n",
      "applies\n",
      "here\n",
      "too!</s>\n",
      "Interoceptive: 0 occurrence(s)\n",
      "Olfactory: 0 occurrence(s)\n",
      "Mouth: 0 occurrence(s)\n",
      "Torso: 0 occurrence(s)\n",
      "Gustatory: 0 occurrence(s)\n",
      "Visual: 0 occurrence(s)\n",
      "Foot_leg: 0 occurrence(s)\n",
      "Auditory: 0 occurrence(s)\n",
      "Hand_arm: 0 occurrence(s)\n",
      "Haptic: 0 occurrence(s)\n",
      "Head: 0 occurrence(s)\n",
      "['T', 'U', 'G', 'R', 'T', 'Z', 'T', 'U', 'G', 'R', 'I', 'Z', 's']\n"
     ]
    }
   ],
   "source": [
    "word_list = reply.split()\n",
    "\n",
    "value_counts = {'Interoceptive': 0, 'Olfactory': 0, 'Mouth': 0, 'Torso': 0, 'Gustatory': 0, 'Visual': 0, 'Foot_leg': 0, 'Auditory': 0, 'Hand_arm': 0, 'Haptic': 0, 'Head': 0}\n",
    "for key in word_list:\n",
    "    print(key)\n",
    "    if key.upper() in shared.sensorimotor:\n",
    "        print('hit')\n",
    "        value_counts[value] += 1\n",
    "\n",
    "for value, count in value_counts.items():\n",
    "    print(f\"{value}: {count} occurrence(s)\")\n",
    "\n",
    "\n",
    "print(first_letters(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'e', 'n', 'r', 's'], ['t', 'e', 's', 'o', 'r'], ['u', 't', 's', 'e', 'i'], ['e', 'i', 't', 'n', 'a'], ['t', 'e', 'r', 'i', 'a'], ['a', 'e', 't', 'n', 'm'], ['e', 'i', 't', 'a', 'o'], ['t', 'e', 'i', 'o', 'h'], ['e', 'n', 'i', 'u', 's'], ['e', 'n', 'i', 'a', 'g'], ['e', 'r', 'a', 'n', 'o'], ['a', 'e', 'n', 't', 'o'], ['e', 't', 'i', 'o', 'p']]\n"
     ]
    }
   ],
   "source": [
    "print(most_common_letters(reply,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'U', 'G', 'R', 'I', 'Z', 'T', 'U', 'G', 'R', 'R', 'Z']\n",
      "[['e', 'n', 'a', 'r', 's'], ['t', 's', 'e', 'n', 'r'], ['e', 't', 'i', 'r', 'o'], ['e', 'i', 't', 'n', 'r'], ['e', 't', 'i', 'r', 'a'], ['a', 'e', 't', 'n', 's'], ['e', 't', 'a', 'o', 'i'], ['t', 'e', 'i', 'n', 'o'], ['e', 'n', 'u', 's', 't'], ['e', 'n', 'a', 'r', 't'], ['e', 'r', 't', 'a', 'o'], ['a', 'e', 'n', 't', 'o'], []]\n"
     ]
    }
   ],
   "source": [
    "translated = 'The 24 Hours of Le Mans is a world-renowned endurance sports car race held annually in Le Mans, France. Unlike most other motorsport events, which usually consist of short sprint races that last only a few minutes or hours, the 24 Hours of Le Mans is a true test for the drivers and their machines, spanning an entire day and night. GT engines are built to maintain high power levels for long periods of time without failure, unlike supercars that have tremendous power but a much shorter life under such conditions. Race cars designed for this competition must be lightweight, aerodynamic and highly efficient in terms of fuel consumption and tire wear. In addition, driver strategy plays a critical role in determining the winner, as teams must carefully manage their drivers changes and pit strategies throughout the race to ensure their car remains competitive from start to finish. Zero waste and maximum performance are essential aspects that make the Le Mans race a fascinating challenge and a great success for any team that can take home the trophy at the end of the race. To achieve the goal of waste reduction, organizations such as the Automobile Club de l Ouest (ACO), which organizes the event, have taken various initiatives to reduce the environmental impact of the race. Understanding these factors underscores the importance of optimizing through technology to achieve the best result with the least negative impact on the environment, while maintaining the thrill and excitement associated with top-level motorsports competition. GT engines used in such events must be carefully tuned and matched to ensure they run efficiently during the grueling duration of the race without causing unnecessarily harmful emissions or excessive fuel consumption. Race regulations also play an important role in encouraging engine designers to develop more energy-efficient engines that produce less waste and pollutants during the race. Research into advanced materials for better aerodynamics or lighter weight supports efforts to create greener racing experiences for future generations. Zero waste and maximum performance are achievable goals that can coexist and even reinforce each other when approached with innovative and forward-thinking approaches that take into account all relevant factors affecting automotive mechanics and dynamics at major racing events such as the 24 Hours of Le Mans.'\n",
    "print(first_letters(translated))\n",
    "print(most_common_letters(translated,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Human: Explain in detail: Where is the country of Austria located? ### Assistant:\n",
      "--------------------\n",
      "Output generated in 15.47 seconds (10.34 tokens/s, 160 tokens, context 20, seed 0)\n",
      "  Austria is a country located at the center of the Western part of the continent of Europe. Exact location coordinates are latitude 47° to 48° North\n",
      "and longitude 13° to 16° East. Switzerland lies directly west of the border while Germany lies directly northwest, Czech Republic borders the east\n",
      "side while Slovenia shares the southern border while the rest are surrounded by other European countries like France, Belgium etc depending on the\n",
      "region one refers to specifically. The Austrian state also includes some islands such as those situated inside its waters such as those surrounding\n",
      "its largest cities such as Vienna itself has several small ones such as Islandstadt Aspern also close to its eastern shore is also shared with\n",
      "Slovakians as its district is also shared across this island Stadt asp</s>\n",
      "['A', 'E', 'S', 'T']\n",
      "[['t', 'e', 'o'], ['t', 'o', 'a'], ['e', 'r', 'i'], ['s', 'a', 't']]\n"
     ]
    }
   ],
   "source": [
    "question = \"### Human: Explain in detail: Where is the country of Austria located? ### Assistant:\"\n",
    "\n",
    "\n",
    "generate_params = {\n",
    "    'max_new_tokens' : 500,\n",
    "    'add_bos_token' : False,\n",
    "    'truncation_length' : 2048,\n",
    "    'custom_stopping_strings' : [\"### Human:\", \"Human:\", \"user:\"],\n",
    "    'ban_eos_token' : False,\n",
    "    'skip_special_tokens' : False,\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.1,\n",
    "    'typical_p': 1,\n",
    "    'repetition_penalty': 1.176,\n",
    "    'encoder_repetition_penalty': 1,\n",
    "    'top_k': 40,\n",
    "    'num_beams': 1,\n",
    "    'penalty_alpha': 0,\n",
    "    'min_length': 0,\n",
    "    'length_penalty': 1,\n",
    "    'no_repeat_ngram_size': 0,\n",
    "    'early_stopping': False,\n",
    "    'seed' : 0,\n",
    "}\n",
    "shared.delta_char = 6.00\n",
    "shared.delta_first = 5000.0\n",
    "shared.code = \"test\"\n",
    "reply = generate_reply(question, generate_params)\n",
    "\n",
    "wrapped_string = \"\\n\".join(textwrap.wrap(reply, width=150))\n",
    "print(wrapped_string)\n",
    "\n",
    "print(first_letters(reply))\n",
    "print(most_common_letters(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Human: Are cats or dogs better in climbing? ### Assistant:\n",
      "--------------------\n",
      "Output generated in 5.40 seconds (14.45 tokens/s, 78 tokens, context 17, seed 0)\n",
      "  It depends on the cat and dog. Some cats are very good at climbing, while others may not be as skilled. Meanwhile, some dogs are also capable of\n",
      "climbing to different heights depending on their breed and training. Effective communication with your child can help them understand that every\n",
      "animal has its own unique abilities and characteristics. Keep reading for more tips!</s>\n"
     ]
    }
   ],
   "source": [
    "question = \"### Human: Are cats or dogs better in climbing? ### Assistant:\"\n",
    "\n",
    "\n",
    "generate_params = {\n",
    "    'max_new_tokens' : 200,\n",
    "    'add_bos_token' : False,\n",
    "    'truncation_length' : 2048,\n",
    "    'custom_stopping_strings' : [\"### Human:\", \"Human:\", \"user:\"],\n",
    "    'ban_eos_token' : False,\n",
    "    'skip_special_tokens' : False,\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.1,\n",
    "    'typical_p': 1,\n",
    "    'repetition_penalty': 1.176,\n",
    "    'encoder_repetition_penalty': 1,\n",
    "    'top_k': 40,\n",
    "    'num_beams': 1,\n",
    "    'penalty_alpha': 0,\n",
    "    'min_length': 0,\n",
    "    'length_penalty': 1,\n",
    "    'no_repeat_ngram_size': 0,\n",
    "    'early_stopping': False,\n",
    "    'seed' : 0,\n",
    "}\n",
    "shared.delta = 6.00\n",
    "shared.delta_first = 5000.0\n",
    "shared.code = \"bsmek\"\n",
    "reply = generate_reply(question, generate_params)\n",
    "\n",
    "wrapped_string = \"\\n\".join(textwrap.wrap(reply, width=150))\n",
    "print(wrapped_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'S', 'M', 'E', 'K', 's']\n",
      "[['d', 't', 'e'], ['e', 's', 'o'], ['e', 'n', 'i'], ['i', 'e', 't'], ['e', 'r', 'p'], ['s']]\n"
     ]
    }
   ],
   "source": [
    "print(first_letters(reply))\n",
    "print(most_common_letters(reply, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76556886ee06879003f8d22dd3357c6a34e06eae87f95849600f64be34531086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
