{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b117300289483786f92f8bf41d67a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1208 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Prompt: You've read about the black box. In the wake of a crash, authorities scramble to find the black box, the key to the investigation.\n",
      "It contains an aircraft's flight data recorder (FDR) and cockpit voice recorder (CVR), which are, as soon as possible, sent off to a lab for analysis, or in a case like the recent Ethiopian Airlines Flight 302 crash, sent to another country better equipped to handle the job — Ethiopian investigators sent the flight's recorders to the French Bureau d'Enquêtes et d'Analyses pour la Sécurité de l'Aviation Civile (BEA). Germany demurred and Ethiopia elected not to send the recorders to American authorities.\n",
      "Major accidents with international interests are particularly sticky, according to Ben Hsu, supervisor of the recorder division at the U.S.'s National Transportation Safety Board (NTSB). The state of occurrence leads the investigation, but Annex 13 of ICAO regulations guarantees a spot on the investigation committee for certain stakeholders, including both countries and companies involved in design, manufacture and registry of both the aircraft and its engine. Additionally, any countries that had a significant number of citizens on board get to send representatives to participate in the investigation.\n",
      "In the case of ET 302, that means the NTSB headed up an American contingent involving the FAA, planemaker Boeing and engine-maker GE to join the Ethiopian-led investigation which, due to the diversity of passengers on board, included at least five fatalities each from Kenya, Canada, China, Italy, France, Germany, the U.K and Egypt.\n",
      "Such major investigations can get unwieldy due to their size, Hsu said, but it's important that all those interests are represented.\n",
      "In a U.S.-led investigation, once the recorders are received at NTSB headquarters in Washington, D.C., which Avionics International visited, they're taken to the multi-room lab at the recorder division for data extraction. It is nominally a 12-member division, though they're short-staffed at \"eight or nine\" and looking to hire right now, Hsu said. How long it takes to extract the data depends on several factors — the current workload, whether the recorders are damaged and the priority of the investigation — but the initial data is usually ready within a day or two of the recorders' arrival.\n",
      "\"Those major investigations get kicked right to the top of the priority list,\" Hsu said. \"Action happens very quickly. They'll be worked on that night if they're received at a reasonable hour or the very next morning.\"\n",
      "The NTSB always has a \"go team\" on standby, a rotating group of on-duty people who will get called into action in the event of an accident, including representatives from each of the necessary departments in the agency. The recorder division is a bit of an exception, because specialties play a big role there.\n",
      "\"Part of my job is, I have to know who is in town and their specialties,\" Hsu said. \"Usually, we need either an FDR or CVR [specialist] and then someone else depending on the type of damage.\"\n",
      "Basically, different skill sets are required if the aircraft — and black box — was downed in the water, went up in flames or sustained impact damage. While one engineer in the recorders division might excel at safely cleaning\n",
      "Baseline Completion:  and drying the hardware, another is more adept at soldering connectors back onto a memory chip without doing unnecessary damage. It's crucial to the investigation that that data can be accessed at least one more time, so having the best person for the job there is a necessity — and, if it turns out that someone different would be a better fit as more is learned, Hsu will sub out members of his team.\n",
      "One engineer in the division, Chris Babcock, said he has developed a few areas of expertise in his decade-plus at the NTSB, but his background is working with acoustics and voice recorders. That comes in handy if the CVR is found but not the FDR. By analyzing the background noise in the cockpit for certain engine or propeller sounds and pitches, Babcock can often tell certain things about their performance that would otherwise be learned from the FDR.\n",
      "This is part one of a two-part story. Look out for the second part next week.\n",
      "==================================================\n",
      "Sample 2:\n",
      "Prompt: Conductor's patient layering delivers beguiling program of baroque and early classical works.\n",
      "Jonathan Cohen sits at a harpsichord when leading the St. Paul Chamber Orchestra in concert, tinkling the keys and occasionally waving his right arm to cue the musicians.\n",
      "To the naked eye he seems to be not doing much, but appearances can be deceptive.\n",
      "Playing as beguiling and exquisitely stylish as the SPCO produced during Saturday evening's program of baroque and early classical music at the Ordway needs scrupulous preparation at rehearsals, and Cohen's influence on the orchestra has been palpable since he joined two years ago as an artistic partner.\n",
      "It showed in the delectably light, transparent textures Cohen achieved with the players in the Fifth Concerto Grosso of Charles Avison, an English contemporary of Handel.\n",
      "The two quicker movements had a nimble light-footedness typical of ballet. In the fugal material of the second slow movement, Cohen's patient layering of the different string lines achieved a quiet intensity that drew the listener inward.\n",
      "Articulation was a touch more emphatic in the busier instrumentation of the fascinating Sinfonia in A minor by the Czech baroque composer Jan Dismas Zelenka.\n",
      "Generous opportunities existed for individual players to take the spotlight. The Andante movement harbored a three-way conversation between oboist William Welter and bassoonist Brad Balliett — both impressing as guest players for the evening — and concertmaster Ruggero Allifranchini.\n",
      "Balliett popped up again in the finale, in chortling conversation with principal cello Julie Albers.\n",
      "Cohen drew highly spirited playing from an ensemble of 20 players, raising an obvious question: Why is a composer of Zelenka's sharp originality so rarely featured on the modern concert platform? The neglect is puzzling.\n",
      "C.P.E. Bach, second surviving son of Johann Sebastian, used to be similarly neglected, but in recent years his star has been steadily rising.\n",
      "The Sinfonia in E minor was a terse reminder of C.P.E.'s almost obsessively inventive music. Cohen drew bracingly taut, articulate phrasing from the orchestra, especially in the knockabout finale, where the skirling violins bolted like a startled jackrabbit.\n",
      "The most delicate playing of the evening came in the concluding item, Haydn's \"Morning\" Symphony, one of a trilogy of works linked to times of the day.\n",
      "Hushed violins\n",
      "Baseline Completion:  heralded the depiction of daybreak at the symphony's opening, and Cohen's unraveling of the gradual crescendo revealing the sunlit scene was picture-perfect.\n",
      "Diaphanously quiet playing also characterized the Adagio movement, providing a soft ambience for the sweet solo musings of first violin Allifranchini, a peerless Haydn stylist.\n",
      "Balliett's affable bassoon resurfaced in the Menuet's trio section, in droll dialogue with the scuttling double bass of Zachary Cohen.\n",
      "Allifranchini's short, zippy cadenza capped a zesty performance of the carefree finale, where the solo breaks of principal flute Julia Bogorad-Kogan provided a garrulous running commentary.\n",
      "Cohen bowed modestly at the symphony's conclusion, deflecting attention to the SPCO players. His specialist input, though, was unmistakable in this heart-lifting, enjoyable concert.\n",
      "==================================================\n",
      "Sample 3:\n",
      "Prompt: Fountain Valley, CA (January 8, 2015)—The Fountain Valley (CA) Chamber of Commerce is extending the deadline for nominations\n",
      "Baseline Completion:  for its annual Business Awards for 2014 until the end of January. The categories for nominations include Elwyn California Community Spirit, Leadership Excellence (replaces the Entrepreneur of the year category), and New Business awards.\n",
      "The deadline for nominations for the 2014 awards is now January 30, 2015.\n",
      "The Chamber is also seeking sponsors for the awards categories and table sponsors, as well as program ads for the awards luncheon. Sponsorship information is available at www.fvchamber.com.\n",
      "The Awards will be presented at the Annual Chamber of Commerce’s Business Awards luncheon to be held on February 24, 2015 at Mile Square Golf Course Banquet Room in Fountain Valley.\n",
      "The nominations are judged by a panel of independent judges from the community selected by the Chamber of Commerce.\n",
      "Details of nomination requirements, nomination forms and other information is available through the Fountain Valley Chamber of Commerce at (714) 962-3822, or at www.fvchamber.com.\n",
      "==================================================\n",
      "Sample 4:\n",
      "Prompt: According to Tom Bower’s book Broken Dreams, Israeli agent Pini Zahavi made £3 million on the transfers of his client Rio Ferdinand from West Ham to Manchester United via Elland Road. Rio’s such a nice bloke that he’s still doing his agent favours, endorsing a new scouting website founded by Zahavi that claims to represent “the world’s biggest ever football database” on players.\n",
      "“IMScouting is an extraordinary database,” says Rio, no doubt spontaneously over a casual pint one night with Pini, who just happened to have a notebook along. “It is indispensable for any football professional looking for a crucial insight into the global game.” In that last sense, he may have a point. Surfing the site, you can’t help but feel that scouting for talent has been reduced to a cursory online shopping cruise. The only thing missing is the “Proceed to checkout” button.\n",
      "IMScouting claims to have “a global network of football experts” to compile its information, but it’s not giving away any more than that. A question emailed to the company’s initially eager PR contact asking who these experts actually are, and where they are based, was ignored. And “global” could mean anything. After all, Wotton-under-Edge is global in the sense that it’s part of the world. Although no one would dare suggest that the site’s content is being researched via Google from a bedroom somewhere in that charming town on the edge of the Cotswolds.\n",
      "Scouring around, it quickly becomes clear that the press release touting the site’s “unprecedented level of detailed information on almost every football player in the world” is tosh. Only Europe, with 26 out of 53 national leagues covered, comes close to meeting the claim. It’s a different story for the regions where the emerging talent is usually scalped by European clubs, and which you’d expect a site like this to target. Out of the whole of Asia, only Japan is covered in detail. In South America, only Brazil and Argentina. Central and North America is a complete blank so far, while Ghana, Egypt and South Africa are the only African leagues covered.\n",
      "Look in more detail, and even these don’t stand up. Almost all the details on Ghanaian team Hearts of Oak are missing, bar the players’ names, age and value. How is the team’s midfielder Thomas Mensah worth £31,000? Do we just take the site’s word for it? Wait a minute, Berekum Arsenal striker Francis Aggrey is also valued at £31k. So is Richard Addai of Ashanti Gold. Has the Ghanaian government fixed the price of football ­players like the price of bread?\n",
      "Let’s try Europe, as at least some players have their career details listed. But how do we know that FC Zurich defender Florian Stahel is worth exactly £793,000? Could it be that a player’s value is calculated by some primary-school computer programme that combines age, appearances and goalscoring stats? His fellow defender Daniel Stucki is worth only £158,000. Is Stahel really five times the player Stucki is? Perhaps, given that Stucki’s “playing style” is listed as “none”.\n",
      "Further evidence of the site’s ham-fisted, premature launch was provided when a new user named Footy Mad Red turned up on the WSC message board last month. He eagerly asked if we’d heard about this fantastic new website, imscouting.com. Shouldn’t we all take a look and get a discussion going on its merits? This kind of viral marketing approach might have duped a few internet newcomers around a decade ago, but the jaundiced regulars, their virtual pitchforks raised high, soon chased Footy Mad Red back to Wotton-under-Edge.\n",
      "So it seems that IMScouting’s not really a project to provide a comprehensive global player database after all, which would in any case have been an ambitious undertaking for the wealthiest of webmasters. In fact it’s just another crummy commercial venture aimed at milking subscription cash out of fantasy football addicts or football agents who haven’t yet worked out how to use YouTube. The first 14 days are free, but at the time of writing there was no way to find out what it would cost after that. Perhaps they’ll work that out the same way they come up with the players’ values: think of a number and ­double it.\n",
      "Too lazy to surf through numerous websites for the latest football stories? Too lazy to\n",
      "Baseline Completion:  even set up your own reader so that all the top stories appear on one page? Fear not, for as long as you or your butler still have the energy to type in a web address, you can find pretty much everything you’ll need to know on a single page set-up that gives you a pleasingly presented, at-a-glance overview of the headlines. There are also separate pages for blogs and audio/video.\n",
      "The downside is that it’s difficult not to feel overwhelmed at all the content when you see so many links laid out on one web-page. On the other hand, even a perfunctory glance reveals that most journalists and bloggers are writing about the same thing, so you’re probably not missing much if you’re selective enough. Though I’m still waiting for the programmer who can devise a system that filters out bad writing and gives me a sparse page entitled The Only Things You Really Need To Read.\n",
      "==================================================\n",
      "Sample 5:\n",
      "Prompt: Facebook CEO Mark Zuckerberg announced today that the social media company will amp up its privacy protections -- welcome news for users sick of having their personal information mined by advertisers. He also, however, announced that targeted ads will become even more targeted.\n",
      "Facebook CEO Mark Zuckerberg announced today that the social media company will amp up its privacy protections — welcome news for users who have come to think of the site as nothing more than personal information mine for advertisers.\n",
      "During a keynote speech at Facebook's F8 Developer Conference, Zuckerberg announced the \"anonymous login,\" as a way for users to maintain more control over how much personal information they'd like to share, when they use their Facebook account to log into other apps and websites.\n",
      "Zuckerberg said during the conference that \"we know some people are scared of pressing [the social login button].... if you're using an app that you don't completely trust... then you don't want to give a lot of permissions.\"\n",
      "Facebook Anonymous Login is a bit like Google's Incognito browsing they introduced inside Chrome. Except, again, it doesn't appear to be 100% anonymous because Facebook still sees the user going anonymous to check out an app. That would be a bit like Chrome keeping you logged into YouTube when you're browsing in Incognito mode.\n",
      "He also explains that if, after using Anonymous Login to download an app, you decide that you like it you still do have to log in with Facebook at some point later on. So it's sort of anonymous... temporarily.\n",
      "The changes, which Facebook says will be adopted by websites and mobile apps within the next year, will give users more choices\n",
      "Baseline Completion:  about the personal information they share with third parties. By checking or unchecking a box, users will be able to specify if they want to share their friend list, their birthday or their “likes,” among others. Currently, people who log in with Facebook Login don’t control the information they share, including their email addresses, their friend lists and other personal data.\n",
      "The new features mean that users will be able to limit the personal information broadcast to other sites and apps that collect data from Facebook, which is bad news for developers. But another feature, the not-unexpected Facebook Audience Network, should appease that demographic.\n",
      "Facebook explains that the Audience Network will help developers \"show your audience ads that matter to them with Facebook's powerful targeting,\" and \"display successful Facebook ads in the format that's right for your app.\"\n",
      "Facebook also introduced AppLinks, which will allow users to move more easily from app to app, and a mobile Like button.\n",
      "==================================================\n",
      "Sample 6:\n",
      "Prompt: Microsoft has spent the first 9 months of the Xbox One being on sale fixing all the issues gamers had with the new machine. It’s now the same price as the PS4, the requirement that a Kinect be attached has gone, and the mandatory bundling of the motion controller has also ended. But Sony has a massive lead, and it looks as though Microsoft is now considering another price cut its console.\n",
      "The Xbox One without Kinect currently sells for 399.99 euros across Europe. However, over the weekend the Spanish version of Xbox.com started displaying a price of 349.99 euros. The equivalent drop in the US would see the price fall from $399 down to just $329.\n",
      "Of course, this could just be a mistake. The lower price only shows up on the mobile version\n",
      "Baseline Completion:  of the Spanish Xbox.com website. But then, why would anyone be messing with the price unless a change was on the cards?\n",
      "The latest figures suggest that the PS4 is outselling the Xbox One 3-to-1 and has an installed user base of 9 million consoles compared to just 5 million for the Xbox One. Just matching the PS4 on price clearly isn’t enough, especially when you consider Sony retains the advantage of offering a more powerful gaming machine.\n",
      "Microsoft can compete by offering exclusive, highly desirable games, but that takes a long time–months, if not years or planning and development. If they want any chance of closing the gap to PS4 quickly, then a price drop is the most obvious thing to do. There’s no guarantee it would work, though, and Microsoft would incur significant losses by doing it so early on in the console’s lifecycle. Sony could also counter, matching any price drop and negating the advantage.\n",
      "==================================================\n",
      "Sample 7:\n",
      "Prompt: The mentioned artwork in X-Men Gold #1 was inserted without knowledge behind its reported meanings. These implied references do not reflect the views of the writer, editors or anyone else at Marvel and are in direct opposition of the inclusiveness of Marvel Comics and what the X-Men have stood for since their creation. This artwork will be removed from subsequent printings, digital versions, and trade paperbacks, and disciplinary action is being taken.\n",
      "According to The New York Times, Marvel decided to pull artwork from the first issue of X-Men Gold and pursue disciplinary action for an Indonesian artist, Ardian Syaf, because he injected Islamist and anti-Semitic symbolism into art panels without the approval or knowledge of the company. The issue in question was supposed to mark the beginning of a reboot in the X-Men franchise, but readers in Indonesia noticed panels throughout the comic book that raised red flags, and they took to social media to voice their concerns about anti-Christian and anti-Semitic symbolism.\n",
      "The social media criticisms indicated that artwork by Ardian Syaf referenced opposition movements against Basuki Tjahaja Purnama, the governor of Jakarta. Purnama, the first Christian to become the governor of Jakarta in decades, is up for re-election. Indonesia has a vast Islamic population, and many of Purnama's opponents believe he is unfit to lead because of his religious beliefs. Critics of X-Men Gold #1 examined how Ardian Syaf, who has worked for Marvel since 2007, added \"QS 5:51\" to the shirt worn by Colossus, one of the X-Men characters in the issue. The critics said \"QS 5:51\" was an apparent reference to Quran chapter Surah 5, verse 51, which some opponents of Governor Purnama have cited and interpreted as to argue against the appointment of Jews and Christians to leadership positions.\n",
      "Another symbolic number, \"212,\" appeared on the front of a store in a panel from the issue. Critics said this number referenced a protest that took place in Jakarta on December 2 of last year--the \"212 protest\"--during which conservative Islamist groups voiced their firm opposition of Purnama. In\n",
      "Baseline Completion:  the same panel featuring \"212,\" critics claimed the word \"Jew\" showed most prominently on the signage of a jewelry store adjacent to Jewish character Kitty Pryde.\n",
      "Syaf previously stated that working for Marvel was a dream come true. In the wake of the controversy, Ardian Syaf posted on social media that he does not hate Christians or Jews; he has since removed the post. Unfortunately, it is too little, too late for Syaf; Marvel announced today that it had terminated its contract with him. His artwork will remain in X-Men Gold #2 and #3 because there is not enough time to recommission the art for the bi-weekly series. However, one of the other artists working on the X-Men Gold project will take over the art for issue #4, which Syaf was originally supposed to do.\n",
      "It will be interesting to see what precautions Marvel takes to avoid similar situations in the future. We'll keep you posted on any updates in this story.\n",
      "==================================================\n",
      "Sample 8:\n",
      "Prompt: Plans are in the works for the largest-ever seismic upgrade of a B.C. school, but some parents are already expressing concerns.\n",
      "The $80-million project was announced back in June, and construction is slated to begin in 2020.\n",
      "Eric Hamber Secondary’s 1,700 students would remain in the old school while the new facility is built, with construction of the LEED gold facility to be completed by 2022.\n",
      "On Friday, parents were invited to an information session, with many saying they did not like what they saw.\n",
      "The major issue is what won’t be included in the project.\n",
      "According to the Hamber Parent Advisory Council (PAC), the new 15,000-square-metre building will be 26 per cent smaller than the existing school.\n",
      "What drew the most concern from parents was what wouldn’t be in the new school.\n",
      "“There are some major deficiencies in the proposed design. There’s no auditorium in the new school,” said Stephanie Yada, who has both a son and daughter at Eric Hamber.\n",
      "She said the proposed gym is also too small, and the new school would lose its track.\n",
      "An information board posted at Friday\\’s event.\n",
      "Florence Lum attended Eric Hamber as a teen, and has two kids in line to attend the school in a few years.\n",
      "The proposed design has her considering moving them cross-boundary.\n",
      "Brenda Chinn, another alumnus and former school president, said any new structure should improve on the facilities that are already there, not cut back.\n",
      "“We want kids to be well-rounded individuals whether its sports, music, arts, theatre and they’re not going to be providing for that any longer,” she said.\n",
      "School board chair Janet Fraser said the primary concerns she heard from parents surrounded gym, auditorium and fashion design spaces.\n",
      "But she said the\n",
      "Baseline Completion:  board must work within the parameters of the Ministry of Education.\n",
      "“We have to work to the ministry Area Standards guidelines, within those guidelines we have to try and design the school in the best possible way for our students,” she said.\n",
      "Those guidelines determine the maximum size for facilities, and the PAC and alumni want to see them reviewed and replaced.\n",
      "“We’re asking that they halt for now, revisit, replace the document or adjust it accordingly, and Hamber students could stay here and going forward have the correct build,” said Andrea Nicholson, alumni coordinator.\n",
      "In a statement, the Ministry of Education said that the VSB is responsible for building the school, and have flexibility when it comes to the design and programs.\n",
      "It added that districts do not have to build school spaces to the exact specification of area standards, and that the main purpose of space allocations are to determine the capital budget, not dictate specific allocations of space within the school.\n",
      "==================================================\n",
      "Sample 9:\n",
      "Prompt: LOS ANGELES — Petfinder.com is looking for at least a million people who want company for Christmas.\n",
      "The online pet-adoption database is teaming up with CBS and the producers of the Hallmark Hall of Fame Movie “A Dog Named Christmas” for a campaign to foster pets for the holidays.\n",
      "“If 10 dogs in every city get a Christmas break, I think that would be fantastic,” added actor Bruce Greenwood, who plays George McCray in the movie, which airs Nov. 29.\n",
      "The book and movie tell the story of a shelter dog taken in by a developmentally challenged young man as part of an “Adopt a Dog for Christmas” campaign. Kincaid made it up for Christmas one year as a bedtime story for his five kids.\n",
      "“The kids hated it,” Kincaid said in a telephone interview from his office in Olathe, Kan.\n",
      "“What kind of Christmas story is that, Dad?” they asked because of the way the story ended. He rewrote the story and read it for them a year later. This version met with the kids' approval and he turned it into a short story, then a novel.\n",
      "The real-life “Foster a Lonely Pet for the Holidays” campaign will draw on 2,080 shelters and rescues across the country for a national foster week, which will last from Christmas Eve through New Year's Day.\n",
      "The idea took hold at a clinic in Pensacola, Fla., that started the campaign last year after the book's release. A nurse there put out a sign that read “Foster a pet for the holidays.” She emptied all the cages for Thanksgiving and again at Christmas, Kincaid said, and they had a waiting list 100 people long.\n",
      "Petfinder.com will provide the shelters with newspaper ads, radio spots and support, said co-founder Betsy Saul.\n",
      "“The smallest act can make such a big difference,” Saul said in a telephone interview from Chapel Hill, N.C.\n",
      "Everyone involved in the movie hopes some animals will find permanent homes through the program. People can use fostering to find out if a pet is a good match or if they are ready for a pet. There are fosters who specialize in older pets, sick pets, pets of a particular breed, kittens and puppies, even pregnant animals.\n",
      "“The beauty\n",
      "Baseline Completion:  of this program is it can be whatever anyone wants it to be,” Saul said.\n",
      "In Virginia, the Charlottesville-Albemarle SPCA plans to pull out all the stops to promote the program and the movie, including fliers, tweets, blogs and calls.\n",
      "“Our goal is to get every pet out,” executive director Susanne Kogut said.\n",
      "No foster effort is a failure, she said, remembering Van, a plain vanilla hound that had been at the shelter for nine months and was overlooked time and again.\n",
      "“We put him in a foster home. He deserved a break. There was nothing distinctive about him. He wasn't overly loud, just a quiet, gentle, wonderful heart,” Kogut said.\n",
      "The foster family had friends over who fell in love with the dog — and just like that he had a forever home.\n",
      "It might not be so bad for the people, either.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Parameters\n",
    "num_samples = 10  # Number of random samples to extract\n",
    "num_tokens_trimmed = 200  # Number of tokens to trim from the end\n",
    "tokenizer_name = \"gpt2\"  # Tokenizer model name\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# Load the RealNews-like subset of the C4 dataset\n",
    "dataset = load_dataset(\"c4\", \"realnewslike\", split=\"train\")\n",
    "\n",
    "# Function to tokenize and process text\n",
    "def process_text(text, num_tokens_trimmed):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Check if there are enough tokens to trim\n",
    "    if len(tokens) <= num_tokens_trimmed:\n",
    "        return None, None\n",
    "    # Split tokens into prompt and baseline completion\n",
    "    prompt_tokens = tokens[:-num_tokens_trimmed]\n",
    "    baseline_tokens = tokens[-num_tokens_trimmed:]\n",
    "    # Convert back to string format\n",
    "    prompt = tokenizer.convert_tokens_to_string(prompt_tokens)\n",
    "    baseline_completion = tokenizer.convert_tokens_to_string(baseline_tokens)\n",
    "    return prompt, baseline_completion\n",
    "\n",
    "# Select random samples\n",
    "sampled_indices = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "# Process each sampled text\n",
    "results = []\n",
    "for idx in sampled_indices:\n",
    "    text = dataset[idx][\"text\"]\n",
    "    prompt, baseline_completion = process_text(text, num_tokens_trimmed)\n",
    "    if prompt and baseline_completion:\n",
    "        results.append({\"prompt\": prompt, \"baseline_completion\": baseline_completion})\n",
    "\n",
    "# Display the results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Prompt:\", result[\"prompt\"])\n",
    "    print(\"Baseline Completion:\", result[\"baseline_completion\"])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word percentages added and saved to updated_word_frequencies_with_percent.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google_ngram_downloader import readline_google_store\n",
    "import os\n",
    "os.chdir(\"/home/feline/master-generation\")\n",
    "\n",
    "\n",
    "# Load your CSV file\n",
    "csv_path = 'Lancaster_sensorimotor_norms_for_39707_words.csv'  # Replace with the path to your CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the .txt file with word frequencies\n",
    "txt_file_path = 'frequency-alpha-alldicts.txt'  # Replace with the path to your .txt file\n",
    "frequencies = {}\n",
    "\n",
    "\n",
    "# Read the .txt file and parse it\n",
    "with open(txt_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\"#\") or line.strip() == \"\":  # Skip header or empty lines\n",
    "            continue\n",
    "        \n",
    "        parts = line.split()\n",
    "        rank = parts[0]  # Ignore the rank\n",
    "        word = parts[1].lower()  # Word is in lowercase\n",
    "        percent = float(parts[3].replace('%', '')) / 100  # Convert percent to decimal form\n",
    "        \n",
    "        frequencies[word] = percent\n",
    "\n",
    "# Function to lookup the word in the frequencies dictionary (case-insensitive)\n",
    "def get_word_percent(word):\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in frequencies:\n",
    "        return '{:.8f}'.format(frequencies[word_lower])  # Format to avoid scientific notation\n",
    "    else:\n",
    "        return '0.00000000'  # Return 0 if the word is not found\n",
    "\n",
    "# Add Percent column to the DataFrame\n",
    "df['Word_Percent'] = df['Word'].apply(lambda word: get_word_percent(word))\n",
    "\n",
    "# Save the updated DataFrame to a new CSV\n",
    "output_csv = 'updated_word_frequencies_with_percent.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Word percentages added and saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model to models/meta-llama_Meta-Llama-3.1-8B\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "README.md: 100%|██████████████████████████████████████████████████| 39.9k/39.9k [00:00<00:00, 411kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                   initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/std.py:1169\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[0;32m-> 1169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/_base.py:609\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 321\u001b[0m\n\u001b[1;32m    318\u001b[0m     downloader\u001b[38;5;241m.\u001b[39mcheck_model_files(model, branch, links, sha256, output_folder)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     \u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msha256\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecific_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecific_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_llamacpp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_llamacpp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 264\u001b[0m, in \u001b[0;36mModelDownloader.download_model_files\u001b[0;34m(self, model, branch, links, sha256, output_folder, progress_bar, start_from_scratch, threads, specific_file, is_llamacpp)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading the model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_download_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_from_scratch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_from_scratch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 240\u001b[0m, in \u001b[0;36mModelDownloader.start_download_threads\u001b[0;34m(self, file_list, output_folder, start_from_scratch, threads)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_download_threads\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_list, output_folder, start_from_scratch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m--> 240\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_from_scratch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_from_scratch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tqdm_class(ex\u001b[38;5;241m.\u001b[39mmap(fn, \u001b[38;5;241m*\u001b[39miterables, chunksize\u001b[38;5;241m=\u001b[39mchunksize), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/_base.py:637\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/feline/master-generation\")\n",
    "\n",
    "# Cell 1: Import necessary libraries\n",
    "import base64\n",
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import requests\n",
    "import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import ConnectionError, RequestException, Timeout\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "base = os.environ.get(\"HF_ENDPOINT\") or \"https://huggingface.co\"\n",
    "\n",
    "# Cell 2: Define the ModelDownloader class\n",
    "\n",
    "class ModelDownloader:\n",
    "    def __init__(self, max_retries=5):\n",
    "        self.max_retries = max_retries\n",
    "        self.session = self.get_session()\n",
    "\n",
    "    def get_session(self):\n",
    "        session = requests.Session()\n",
    "        if self.max_retries:\n",
    "            session.mount('https://cdn-lfs.huggingface.co', HTTPAdapter(max_retries=self.max_retries))\n",
    "            session.mount('https://huggingface.co', HTTPAdapter(max_retries=self.max_retries))\n",
    "\n",
    "        if os.getenv('HF_USER') is not None and os.getenv('HF_PASS') is not None:\n",
    "            session.auth = (os.getenv('HF_USER'), os.getenv('HF_PASS'))\n",
    "\n",
    "        try:\n",
    "            from huggingface_hub import get_token\n",
    "            token = get_token()\n",
    "        except ImportError:\n",
    "            token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "        if token is not None:\n",
    "            session.headers = {'authorization': f'Bearer {token}'}\n",
    "\n",
    "        return session\n",
    "\n",
    "    def sanitize_model_and_branch_names(self, model, branch):\n",
    "        if model[-1] == '/':\n",
    "            model = model[:-1]\n",
    "\n",
    "        if model.startswith(base + '/'):\n",
    "            model = model[len(base) + 1:]\n",
    "\n",
    "        model_parts = model.split(\":\")\n",
    "        model = model_parts[0] if len(model_parts) > 0 else model\n",
    "        branch = model_parts[1] if len(model_parts) > 1 else branch\n",
    "\n",
    "        if branch is None:\n",
    "            branch = \"main\"\n",
    "        else:\n",
    "            pattern = re.compile(r\"^[a-zA-Z0-9._-]+$\")\n",
    "            if not pattern.match(branch):\n",
    "                raise ValueError(\n",
    "                    \"Invalid branch name. Only alphanumeric characters, period, underscore and dash are allowed.\")\n",
    "\n",
    "        return model, branch\n",
    "\n",
    "    def get_download_links_from_huggingface(self, model, branch, text_only=False, specific_file=None):\n",
    "        session = self.session\n",
    "        page = f\"/api/models/{model}/tree/{branch}\"\n",
    "        cursor = b\"\"\n",
    "\n",
    "        links = []\n",
    "        sha256 = []\n",
    "        classifications = []\n",
    "        has_pytorch = False\n",
    "        has_pt = False\n",
    "        has_gguf = False\n",
    "        has_safetensors = False\n",
    "        is_lora = False\n",
    "        while True:\n",
    "            url = f\"{base}{page}\" + (f\"?cursor={cursor.decode()}\" if cursor else \"\")\n",
    "            r = session.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            content = r.content\n",
    "\n",
    "            dict = json.loads(content)\n",
    "            if len(dict) == 0:\n",
    "                break\n",
    "\n",
    "            for i in range(len(dict)):\n",
    "                fname = dict[i]['path']\n",
    "                if specific_file not in [None, ''] and fname != specific_file:\n",
    "                    continue\n",
    "\n",
    "                if not is_lora and fname.endswith(('adapter_config.json', 'adapter_model.bin')):\n",
    "                    is_lora = True\n",
    "\n",
    "                is_pytorch = re.match(r\"(pytorch|adapter|gptq)_model.*\\.bin\", fname)\n",
    "                is_safetensors = re.match(r\".*\\.safetensors\", fname)\n",
    "                is_pt = re.match(r\".*\\.pt\", fname)\n",
    "                is_gguf = re.match(r'.*\\.gguf', fname)\n",
    "                is_tiktoken = re.match(r\".*\\.tiktoken\", fname)\n",
    "                is_tokenizer = re.match(r\"(tokenizer|ice|spiece).*\\.model\", fname) or is_tiktoken\n",
    "                is_text = re.match(r\".*\\.(txt|json|py|md)\", fname) or is_tokenizer\n",
    "                if any((is_pytorch, is_safetensors, is_pt, is_gguf, is_tokenizer, is_text)):\n",
    "                    if 'lfs' in dict[i]:\n",
    "                        sha256.append([fname, dict[i]['lfs']['oid']])\n",
    "\n",
    "                    if is_text:\n",
    "                        links.append(f\"{base}/{model}/resolve/{branch}/{fname}\")\n",
    "                        classifications.append('text')\n",
    "                        continue\n",
    "\n",
    "                    if not text_only:\n",
    "                        links.append(f\"{base}/{model}/resolve/{branch}/{fname}\")\n",
    "                        if is_safetensors:\n",
    "                            has_safetensors = True\n",
    "                            classifications.append('safetensors')\n",
    "                        elif is_pytorch:\n",
    "                            has_pytorch = True\n",
    "                            classifications.append('pytorch')\n",
    "                        elif is_pt:\n",
    "                            has_pt = True\n",
    "                            classifications.append('pt')\n",
    "                        elif is_gguf:\n",
    "                            has_gguf = True\n",
    "                            classifications.append('gguf')\n",
    "\n",
    "            cursor = base64.b64encode(f'{{\"file_name\":\"{dict[-1][\"path\"]}\"}}'.encode()) + b':50'\n",
    "            cursor = base64.b64encode(cursor)\n",
    "            cursor = cursor.replace(b'=', b'%3D')\n",
    "\n",
    "        if (has_pytorch or has_pt or has_gguf) and has_safetensors:\n",
    "            has_gguf = False\n",
    "            for i in range(len(classifications) - 1, -1, -1):\n",
    "                if classifications[i] in ['pytorch', 'pt', 'gguf']:\n",
    "                    links.pop(i)\n",
    "\n",
    "        if has_gguf and specific_file is None:\n",
    "            has_q4km = False\n",
    "            for i in range(len(classifications) - 1, -1, -1):\n",
    "                if 'q4_k_m' in links[i].lower():\n",
    "                    has_q4km = True\n",
    "\n",
    "            if has_q4km:\n",
    "                for i in range(len(classifications) - 1, -1, -1):\n",
    "                    if 'q4_k_m' not in links[i].lower():\n",
    "                        links.pop(i)\n",
    "            else:\n",
    "                for i in range(len(classifications) - 1, -1, -1):\n",
    "                    if links[i].lower().endswith('.gguf'):\n",
    "                        links.pop(i)\n",
    "\n",
    "        is_llamacpp = has_gguf and specific_file is not None\n",
    "        return links, sha256, is_lora, is_llamacpp\n",
    "\n",
    "    def get_output_folder(self, model, branch, is_lora, is_llamacpp=False, model_dir=None):\n",
    "        if model_dir:\n",
    "            base_folder = model_dir\n",
    "        else:\n",
    "            base_folder = 'models' if not is_lora else 'loras'\n",
    "\n",
    "        if is_llamacpp:\n",
    "            return Path(base_folder)\n",
    "\n",
    "        output_folder = f\"{'_'.join(model.split('/')[-2:])}\"\n",
    "        if branch != 'main':\n",
    "            output_folder += f'_{branch}'\n",
    "\n",
    "        output_folder = Path(base_folder) / output_folder\n",
    "        return output_folder\n",
    "\n",
    "    def get_single_file(self, url, output_folder, start_from_scratch=False):\n",
    "        filename = Path(url.rsplit('/', 1)[1])\n",
    "        output_path = output_folder / filename\n",
    "\n",
    "        max_retries = 7\n",
    "        attempt = 0\n",
    "        while attempt < max_retries:\n",
    "            attempt += 1\n",
    "            session = self.session\n",
    "            headers = {}\n",
    "            mode = 'wb'\n",
    "\n",
    "            try:\n",
    "                if output_path.exists() and not start_from_scratch:\n",
    "                    r = session.get(url, stream=True, timeout=20)\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    if output_path.stat().st_size >= total_size:\n",
    "                        return\n",
    "\n",
    "                    headers = {'Range': f'bytes={output_path.stat().st_size}-'}\n",
    "                    mode = 'ab'\n",
    "\n",
    "                with session.get(url, stream=True, headers=headers, timeout=30) as r:\n",
    "                    r.raise_for_status()\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    block_size = 1024 * 1024\n",
    "\n",
    "                    filename_str = str(filename)\n",
    "\n",
    "                    tqdm_kwargs = {\n",
    "                        'total': total_size,\n",
    "                        'unit': 'B',\n",
    "                        'unit_scale': True,\n",
    "                        'unit_divisor': 1024,\n",
    "                        'bar_format': '{desc}{percentage:3.0f}%|{bar:50}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n",
    "                        'desc': f\"{filename_str}: \"\n",
    "                    }\n",
    "\n",
    "                    if 'COLAB_GPU' in os.environ:\n",
    "                        tqdm_kwargs.update({\n",
    "                            'position': 0,\n",
    "                            'leave': True\n",
    "                        })\n",
    "\n",
    "                    with open(output_path, mode) as f:\n",
    "                        with tqdm.tqdm(**tqdm_kwargs) as t:\n",
    "                            count = 0\n",
    "                            for data in r.iter_content(block_size):\n",
    "                                f.write(data)\n",
    "                                t.update(len(data))\n",
    "                                if total_size != 0 and self.progress_bar is not None:\n",
    "                                    count += len(data)\n",
    "                                    self.progress_bar(float(count) / float(total_size), f\"{filename_str}\")\n",
    "\n",
    "                    break\n",
    "            except (RequestException, ConnectionError, Timeout) as e:\n",
    "                print(f\"Error downloading {filename}: {e}.\")\n",
    "                print(f\"That was attempt {attempt}/{max_retries}.\", end=' ')\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Retry begins in {2 ** attempt} seconds.\")\n",
    "                    sleep(2 ** attempt)\n",
    "                else:\n",
    "                    print(\"Failed to download after the maximum number of attempts.\")\n",
    "\n",
    "    def start_download_threads(self, file_list, output_folder, start_from_scratch=False, threads=4):\n",
    "        thread_map(lambda url: self.get_single_file(url, output_folder, start_from_scratch=start_from_scratch), file_list, max_workers=threads, disable=True)\n",
    "\n",
    "    def download_model_files(self, model, branch, links, sha256, output_folder, progress_bar=None, start_from_scratch=False, threads=4, specific_file=None, is_llamacpp=False):\n",
    "        self.progress_bar = progress_bar\n",
    "\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if not is_llamacpp:\n",
    "            metadata = f'url: https://huggingface.co/{model}\\n' \\\n",
    "                       f'branch: {branch}\\n' \\\n",
    "                       f'download date: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n'\n",
    "\n",
    "            sha256_str = '\\n'.join([f'    {item[1]} {item[0]}' for item in sha256])\n",
    "            if sha256_str:\n",
    "                metadata += f'sha256sum:\\n{sha256_str}'\n",
    "\n",
    "            metadata += '\\n'\n",
    "            (output_folder / 'huggingface-metadata.txt').write_text(metadata)\n",
    "\n",
    "        if specific_file:\n",
    "            print(f\"Downloading {specific_file} to {output_folder}\")\n",
    "        else:\n",
    "            print(f\"Downloading the model to {output_folder}\")\n",
    "\n",
    "        self.start_download_threads(links, output_folder, start_from_scratch=start_from_scratch, threads=threads)\n",
    "\n",
    "    def check_model_files(self, model, branch, links, sha256, output_folder):\n",
    "        validated = True\n",
    "        for i in range(len(sha256)):\n",
    "            fpath = (output_folder / sha256[i][0])\n",
    "\n",
    "            if not fpath.exists():\n",
    "                print(f\"The following file is missing: {fpath}\")\n",
    "                validated = False\n",
    "                continue\n",
    "\n",
    "            with open(output_folder / sha256[i][0], \"rb\") as f:\n",
    "                bytes = f.read()\n",
    "                file_hash = hashlib.sha256(bytes).hexdigest()\n",
    "                if file_hash != sha256[i][1]:\n",
    "                    print(f'Checksum failed: {sha256[i][0]}  {sha256[i][1]}')\n",
    "                    validated = False\n",
    "                else:\n",
    "                    print(f'Checksum validated: {sha256[i][0]}  {sha256[i][1]}')\n",
    "\n",
    "        if validated:\n",
    "            print('[+] Validated checksums of all model files!')\n",
    "        else:\n",
    "            print('[-] Invalid checksums. Rerun download-model.py with the --clean flag.')\n",
    "\n",
    "# Cell 3: Define input parameters and run the downloader\n",
    "\n",
    "# Instead of argparse, directly define the arguments here\n",
    "model = 'meta-llama/Meta-Llama-3.1-8B'  # Example: You can change this\n",
    "branch = 'main'\n",
    "threads = 4\n",
    "text_only = False\n",
    "specific_file = None\n",
    "output = None\n",
    "model_dir = None\n",
    "clean = False\n",
    "check = False\n",
    "max_retries = 5\n",
    "\n",
    "# Initialize the downloader\n",
    "downloader = ModelDownloader(max_retries=max_retries)\n",
    "\n",
    "# Clean up the model/branch names\n",
    "model, branch = downloader.sanitize_model_and_branch_names(model, branch)\n",
    "\n",
    "# Get the download links from Hugging Face\n",
    "links, sha256, is_lora, is_llamacpp = downloader.get_download_links_from_huggingface(model, branch, text_only=text_only, specific_file=specific_file)\n",
    "\n",
    "# Get the output folder\n",
    "output_folder = downloader.get_output_folder(model, branch, is_lora, is_llamacpp=is_llamacpp, model_dir=model_dir)\n",
    "\n",
    "if check:\n",
    "    # Check previously downloaded files\n",
    "    downloader.check_model_files(model, branch, links, sha256, output_folder)\n",
    "else:\n",
    "    # Download files\n",
    "    downloader.download_model_files(model, branch, links, sha256, output_folder, threads=threads, specific_file=specific_file, is_llamacpp=is_llamacpp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "32000\n",
      "LlamaTokenizer (LLaMA 2 or earlier):\n",
      "Tokens: <s>This is a test sentence for tokenization comparison. How do you like it?\n",
      "Token IDs: [1, 851, 349, 264, 1369, 12271, 354, 6029, 1837, 10367, 28723, 1602, 511, 368, 737, 378, 28804]\n",
      "\n",
      "AutoTokenizer (LLaMA 3):\n",
      "Tokens: [1, 1, 28705, 851, 349, 264, 1369, 12271, 354, 6029, 1837, 10367, 28723, 1602, 511, 368, 737, 378, 28804]\n",
      "Token IDs: [1, 851, 349, 264, 1369, 12271, 354, 6029, 1837, 10367, 28723, 1602, 511, 368, 737, 378, 28804]\n",
      "33 tokens start with 'X' in LlamaTokenizer (LLaMA 2)\n",
      "33 tokens start with 'X' in AutoTokenizer (LLaMA 3)\n",
      "33 tokens start with 'Q' in LlamaTokenizer (LLaMA 2)\n",
      "33 tokens start with 'Q' in AutoTokenizer (LLaMA 3)\n",
      "32000\n",
      "32000\n",
      "The lists are not identical\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[18674, 26125, 10886, 19441, 21894, 22135, 11377, 5580, 2682, 10510]\n",
      "[31990, 31991, 31992, 31993, 31994, 31995, 31996, 31997, 31998, 31999]\n",
      "[15297, 14026, 30131, 26791, 31362, 4817, 23476, 21378, 23568, 2371]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, AutoTokenizer, LlamaTokenizerFast, PreTrainedTokenizerFast\n",
    "import os\n",
    "os.chdir(\"/home/feline/master-generation\")\n",
    "\n",
    "def count_tokens_starting_with_Q(tokenizer, tokenizer_name):\n",
    "    count = 0\n",
    "    \n",
    "    # Loop over the tokenizer's vocabulary\n",
    "    for token_id in range(tokenizer.vocab_size):\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        \n",
    "        # Check if the token starts with 'X'\n",
    "        if token.startswith('Q') or token.startswith('ĠQ') or token.startswith('▁Q'):\n",
    "            count += 1\n",
    "\n",
    "    print(f\"{count} tokens start with 'X' in {tokenizer_name}\")\n",
    "    return count\n",
    "\n",
    "import re\n",
    "from transformers import LlamaTokenizer, AutoTokenizer\n",
    "\n",
    "def count_tokens_starting_with_X(tokenizer, tokenizer_name):\n",
    "    count = 0\n",
    "    \n",
    "    # Define a regex to remove any non-alphabetic characters from the start of the token\n",
    "    non_alpha_prefix = re.compile(r'^[^a-zA-Z]+')\n",
    "\n",
    "    # Loop over the tokenizer's vocabulary\n",
    "    for token_id in range(tokenizer.vocab_size):\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        \n",
    "        # Remove any non-alphabetic characters at the start of the token\n",
    "        stripped_token = non_alpha_prefix.sub('', token)\n",
    "        \n",
    "        # Check if the cleaned token starts with 'X'\n",
    "        if stripped_token.startswith('Q'):\n",
    "            count += 1\n",
    "\n",
    "    print(f\"{count} tokens start with 'Q' in {tokenizer_name}\")\n",
    "    return count\n",
    "\n",
    "# Define your text input\n",
    "text_input = \"This is a test sentence for tokenization comparison. How do you like it?\"\n",
    "\n",
    "# Load LlamaTokenizer (for LLaMA 2 or earlier)\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained('models/dolphin-2.6-mistral-7b-Mistral-7B-Instruct-v0.1')\n",
    "\n",
    "\n",
    "# Load AutoTokenizer (for LLaMA 3)\n",
    "#auto_tokenizer = PreTrainedTokenizerFast.from_pretrained('models/Meta-Llama-3.1-8B')\n",
    "auto_tokenizer = LlamaTokenizerFast.from_pretrained('models/dolphin-2.6-mistral-7b-Mistral-7B-Instruct-v0.1')\n",
    "auto_tokenizer.encode_special_tokens = True\n",
    "\n",
    "print(len(llama_tokenizer))\n",
    "print(len(auto_tokenizer))\n",
    "\n",
    "\n",
    "# Tokenize the input using LlamaTokenizer\n",
    "llama_tokens = llama_tokenizer.tokenize(text_input)\n",
    "llama_token_ids = llama_tokenizer.encode(text_input)\n",
    "\n",
    "# Tokenize the input using AutoTokenizer\n",
    "auto_tokens = auto_tokenizer.tokenize(text_input)\n",
    "auto_token_ids = auto_tokenizer.encode(text_input)\n",
    "\n",
    "# Print results for comparison\n",
    "print(\"LlamaTokenizer (LLaMA 2 or earlier):\")\n",
    "print(f\"Tokens: {llama_tokenizer.decode(llama_token_ids)}\")\n",
    "print(f\"Token IDs: {llama_token_ids}\")\n",
    "\n",
    "print(\"\\nAutoTokenizer (LLaMA 3):\")\n",
    "print(f\"Tokens: {auto_tokenizer.encode(auto_tokenizer.decode(auto_token_ids))}\")\n",
    "print(f\"Token IDs: {auto_token_ids}\")\n",
    "\n",
    "\n",
    "# Count tokens that start with \"Q\" in both tokenizers\n",
    "count_tokens_starting_with_Q(llama_tokenizer, 'LlamaTokenizer (LLaMA 2)')\n",
    "count_tokens_starting_with_Q(auto_tokenizer, 'AutoTokenizer (LLaMA 3)')\n",
    "\n",
    "# Count tokens that start with \"X\" in both tokenizers\n",
    "count_tokens_starting_with_X(llama_tokenizer, 'LlamaTokenizer (LLaMA 2)')\n",
    "count_tokens_starting_with_X(auto_tokenizer, 'AutoTokenizer (LLaMA 3)')\n",
    "\n",
    "\n",
    "llama_tokenizer.add_bos_token = False\n",
    "auto_tokenizer.add_bos_token = False\n",
    "vocab=list(llama_tokenizer.get_vocab().values())\n",
    "vocabfast=list(auto_tokenizer.get_vocab().values())\n",
    "print(len(vocab))\n",
    "print(len(vocabfast))\n",
    "if vocab == vocabfast:\n",
    "    print(\"The lists are identical\")\n",
    "else:\n",
    "    print(\"The lists are not identical\")\n",
    "\n",
    "print(vocab[:10])\n",
    "print(vocabfast[:10])\n",
    "\n",
    "print(vocab[-10:])\n",
    "print(vocabfast[-10:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
