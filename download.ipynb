{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word percentages added and saved to updated_word_frequencies_with_percent.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google_ngram_downloader import readline_google_store\n",
    "import os\n",
    "os.chdir(\"/home/feline/master-generation\")\n",
    "\n",
    "\n",
    "# Load your CSV file\n",
    "csv_path = 'Lancaster_sensorimotor_norms_for_39707_words.csv'  # Replace with the path to your CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the .txt file with word frequencies\n",
    "txt_file_path = 'frequency-alpha-alldicts.txt'  # Replace with the path to your .txt file\n",
    "frequencies = {}\n",
    "\n",
    "\n",
    "# Read the .txt file and parse it\n",
    "with open(txt_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\"#\") or line.strip() == \"\":  # Skip header or empty lines\n",
    "            continue\n",
    "        \n",
    "        parts = line.split()\n",
    "        rank = parts[0]  # Ignore the rank\n",
    "        word = parts[1].lower()  # Word is in lowercase\n",
    "        percent = float(parts[3].replace('%', '')) / 100  # Convert percent to decimal form\n",
    "        \n",
    "        frequencies[word] = percent\n",
    "\n",
    "# Function to lookup the word in the frequencies dictionary (case-insensitive)\n",
    "def get_word_percent(word):\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in frequencies:\n",
    "        return '{:.8f}'.format(frequencies[word_lower])  # Format to avoid scientific notation\n",
    "    else:\n",
    "        return '0.00000000'  # Return 0 if the word is not found\n",
    "\n",
    "# Add Percent column to the DataFrame\n",
    "df['Word_Percent'] = df['Word'].apply(lambda word: get_word_percent(word))\n",
    "\n",
    "# Save the updated DataFrame to a new CSV\n",
    "output_csv = 'updated_word_frequencies_with_percent.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Word percentages added and saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model to models/meta-llama_Meta-Llama-3.1-8B\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "README.md: 100%|██████████████████████████████████████████████████| 39.9k/39.9k [00:00<00:00, 411kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                   initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/std.py:1169\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[0;32m-> 1169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/_base.py:609\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 321\u001b[0m\n\u001b[1;32m    318\u001b[0m     downloader\u001b[38;5;241m.\u001b[39mcheck_model_files(model, branch, links, sha256, output_folder)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     \u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msha256\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecific_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecific_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_llamacpp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_llamacpp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 264\u001b[0m, in \u001b[0;36mModelDownloader.download_model_files\u001b[0;34m(self, model, branch, links, sha256, output_folder, progress_bar, start_from_scratch, threads, specific_file, is_llamacpp)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading the model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_download_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_from_scratch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_from_scratch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 240\u001b[0m, in \u001b[0;36mModelDownloader.start_download_threads\u001b[0;34m(self, file_list, output_folder, start_from_scratch, threads)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_download_threads\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_list, output_folder, start_from_scratch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m--> 240\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_from_scratch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_from_scratch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tqdm_class(ex\u001b[38;5;241m.\u001b[39mmap(fn, \u001b[38;5;241m*\u001b[39miterables, chunksize\u001b[38;5;241m=\u001b[39mchunksize), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/_base.py:637\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/feline/master-generation\")\n",
    "\n",
    "# Cell 1: Import necessary libraries\n",
    "import base64\n",
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import requests\n",
    "import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import ConnectionError, RequestException, Timeout\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "base = os.environ.get(\"HF_ENDPOINT\") or \"https://huggingface.co\"\n",
    "\n",
    "# Cell 2: Define the ModelDownloader class\n",
    "\n",
    "class ModelDownloader:\n",
    "    def __init__(self, max_retries=5):\n",
    "        self.max_retries = max_retries\n",
    "        self.session = self.get_session()\n",
    "\n",
    "    def get_session(self):\n",
    "        session = requests.Session()\n",
    "        if self.max_retries:\n",
    "            session.mount('https://cdn-lfs.huggingface.co', HTTPAdapter(max_retries=self.max_retries))\n",
    "            session.mount('https://huggingface.co', HTTPAdapter(max_retries=self.max_retries))\n",
    "\n",
    "        if os.getenv('HF_USER') is not None and os.getenv('HF_PASS') is not None:\n",
    "            session.auth = (os.getenv('HF_USER'), os.getenv('HF_PASS'))\n",
    "\n",
    "        try:\n",
    "            from huggingface_hub import get_token\n",
    "            token = get_token()\n",
    "        except ImportError:\n",
    "            token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "        if token is not None:\n",
    "            session.headers = {'authorization': f'Bearer {token}'}\n",
    "\n",
    "        return session\n",
    "\n",
    "    def sanitize_model_and_branch_names(self, model, branch):\n",
    "        if model[-1] == '/':\n",
    "            model = model[:-1]\n",
    "\n",
    "        if model.startswith(base + '/'):\n",
    "            model = model[len(base) + 1:]\n",
    "\n",
    "        model_parts = model.split(\":\")\n",
    "        model = model_parts[0] if len(model_parts) > 0 else model\n",
    "        branch = model_parts[1] if len(model_parts) > 1 else branch\n",
    "\n",
    "        if branch is None:\n",
    "            branch = \"main\"\n",
    "        else:\n",
    "            pattern = re.compile(r\"^[a-zA-Z0-9._-]+$\")\n",
    "            if not pattern.match(branch):\n",
    "                raise ValueError(\n",
    "                    \"Invalid branch name. Only alphanumeric characters, period, underscore and dash are allowed.\")\n",
    "\n",
    "        return model, branch\n",
    "\n",
    "    def get_download_links_from_huggingface(self, model, branch, text_only=False, specific_file=None):\n",
    "        session = self.session\n",
    "        page = f\"/api/models/{model}/tree/{branch}\"\n",
    "        cursor = b\"\"\n",
    "\n",
    "        links = []\n",
    "        sha256 = []\n",
    "        classifications = []\n",
    "        has_pytorch = False\n",
    "        has_pt = False\n",
    "        has_gguf = False\n",
    "        has_safetensors = False\n",
    "        is_lora = False\n",
    "        while True:\n",
    "            url = f\"{base}{page}\" + (f\"?cursor={cursor.decode()}\" if cursor else \"\")\n",
    "            r = session.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            content = r.content\n",
    "\n",
    "            dict = json.loads(content)\n",
    "            if len(dict) == 0:\n",
    "                break\n",
    "\n",
    "            for i in range(len(dict)):\n",
    "                fname = dict[i]['path']\n",
    "                if specific_file not in [None, ''] and fname != specific_file:\n",
    "                    continue\n",
    "\n",
    "                if not is_lora and fname.endswith(('adapter_config.json', 'adapter_model.bin')):\n",
    "                    is_lora = True\n",
    "\n",
    "                is_pytorch = re.match(r\"(pytorch|adapter|gptq)_model.*\\.bin\", fname)\n",
    "                is_safetensors = re.match(r\".*\\.safetensors\", fname)\n",
    "                is_pt = re.match(r\".*\\.pt\", fname)\n",
    "                is_gguf = re.match(r'.*\\.gguf', fname)\n",
    "                is_tiktoken = re.match(r\".*\\.tiktoken\", fname)\n",
    "                is_tokenizer = re.match(r\"(tokenizer|ice|spiece).*\\.model\", fname) or is_tiktoken\n",
    "                is_text = re.match(r\".*\\.(txt|json|py|md)\", fname) or is_tokenizer\n",
    "                if any((is_pytorch, is_safetensors, is_pt, is_gguf, is_tokenizer, is_text)):\n",
    "                    if 'lfs' in dict[i]:\n",
    "                        sha256.append([fname, dict[i]['lfs']['oid']])\n",
    "\n",
    "                    if is_text:\n",
    "                        links.append(f\"{base}/{model}/resolve/{branch}/{fname}\")\n",
    "                        classifications.append('text')\n",
    "                        continue\n",
    "\n",
    "                    if not text_only:\n",
    "                        links.append(f\"{base}/{model}/resolve/{branch}/{fname}\")\n",
    "                        if is_safetensors:\n",
    "                            has_safetensors = True\n",
    "                            classifications.append('safetensors')\n",
    "                        elif is_pytorch:\n",
    "                            has_pytorch = True\n",
    "                            classifications.append('pytorch')\n",
    "                        elif is_pt:\n",
    "                            has_pt = True\n",
    "                            classifications.append('pt')\n",
    "                        elif is_gguf:\n",
    "                            has_gguf = True\n",
    "                            classifications.append('gguf')\n",
    "\n",
    "            cursor = base64.b64encode(f'{{\"file_name\":\"{dict[-1][\"path\"]}\"}}'.encode()) + b':50'\n",
    "            cursor = base64.b64encode(cursor)\n",
    "            cursor = cursor.replace(b'=', b'%3D')\n",
    "\n",
    "        if (has_pytorch or has_pt or has_gguf) and has_safetensors:\n",
    "            has_gguf = False\n",
    "            for i in range(len(classifications) - 1, -1, -1):\n",
    "                if classifications[i] in ['pytorch', 'pt', 'gguf']:\n",
    "                    links.pop(i)\n",
    "\n",
    "        if has_gguf and specific_file is None:\n",
    "            has_q4km = False\n",
    "            for i in range(len(classifications) - 1, -1, -1):\n",
    "                if 'q4_k_m' in links[i].lower():\n",
    "                    has_q4km = True\n",
    "\n",
    "            if has_q4km:\n",
    "                for i in range(len(classifications) - 1, -1, -1):\n",
    "                    if 'q4_k_m' not in links[i].lower():\n",
    "                        links.pop(i)\n",
    "            else:\n",
    "                for i in range(len(classifications) - 1, -1, -1):\n",
    "                    if links[i].lower().endswith('.gguf'):\n",
    "                        links.pop(i)\n",
    "\n",
    "        is_llamacpp = has_gguf and specific_file is not None\n",
    "        return links, sha256, is_lora, is_llamacpp\n",
    "\n",
    "    def get_output_folder(self, model, branch, is_lora, is_llamacpp=False, model_dir=None):\n",
    "        if model_dir:\n",
    "            base_folder = model_dir\n",
    "        else:\n",
    "            base_folder = 'models' if not is_lora else 'loras'\n",
    "\n",
    "        if is_llamacpp:\n",
    "            return Path(base_folder)\n",
    "\n",
    "        output_folder = f\"{'_'.join(model.split('/')[-2:])}\"\n",
    "        if branch != 'main':\n",
    "            output_folder += f'_{branch}'\n",
    "\n",
    "        output_folder = Path(base_folder) / output_folder\n",
    "        return output_folder\n",
    "\n",
    "    def get_single_file(self, url, output_folder, start_from_scratch=False):\n",
    "        filename = Path(url.rsplit('/', 1)[1])\n",
    "        output_path = output_folder / filename\n",
    "\n",
    "        max_retries = 7\n",
    "        attempt = 0\n",
    "        while attempt < max_retries:\n",
    "            attempt += 1\n",
    "            session = self.session\n",
    "            headers = {}\n",
    "            mode = 'wb'\n",
    "\n",
    "            try:\n",
    "                if output_path.exists() and not start_from_scratch:\n",
    "                    r = session.get(url, stream=True, timeout=20)\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    if output_path.stat().st_size >= total_size:\n",
    "                        return\n",
    "\n",
    "                    headers = {'Range': f'bytes={output_path.stat().st_size}-'}\n",
    "                    mode = 'ab'\n",
    "\n",
    "                with session.get(url, stream=True, headers=headers, timeout=30) as r:\n",
    "                    r.raise_for_status()\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    block_size = 1024 * 1024\n",
    "\n",
    "                    filename_str = str(filename)\n",
    "\n",
    "                    tqdm_kwargs = {\n",
    "                        'total': total_size,\n",
    "                        'unit': 'B',\n",
    "                        'unit_scale': True,\n",
    "                        'unit_divisor': 1024,\n",
    "                        'bar_format': '{desc}{percentage:3.0f}%|{bar:50}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n",
    "                        'desc': f\"{filename_str}: \"\n",
    "                    }\n",
    "\n",
    "                    if 'COLAB_GPU' in os.environ:\n",
    "                        tqdm_kwargs.update({\n",
    "                            'position': 0,\n",
    "                            'leave': True\n",
    "                        })\n",
    "\n",
    "                    with open(output_path, mode) as f:\n",
    "                        with tqdm.tqdm(**tqdm_kwargs) as t:\n",
    "                            count = 0\n",
    "                            for data in r.iter_content(block_size):\n",
    "                                f.write(data)\n",
    "                                t.update(len(data))\n",
    "                                if total_size != 0 and self.progress_bar is not None:\n",
    "                                    count += len(data)\n",
    "                                    self.progress_bar(float(count) / float(total_size), f\"{filename_str}\")\n",
    "\n",
    "                    break\n",
    "            except (RequestException, ConnectionError, Timeout) as e:\n",
    "                print(f\"Error downloading {filename}: {e}.\")\n",
    "                print(f\"That was attempt {attempt}/{max_retries}.\", end=' ')\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Retry begins in {2 ** attempt} seconds.\")\n",
    "                    sleep(2 ** attempt)\n",
    "                else:\n",
    "                    print(\"Failed to download after the maximum number of attempts.\")\n",
    "\n",
    "    def start_download_threads(self, file_list, output_folder, start_from_scratch=False, threads=4):\n",
    "        thread_map(lambda url: self.get_single_file(url, output_folder, start_from_scratch=start_from_scratch), file_list, max_workers=threads, disable=True)\n",
    "\n",
    "    def download_model_files(self, model, branch, links, sha256, output_folder, progress_bar=None, start_from_scratch=False, threads=4, specific_file=None, is_llamacpp=False):\n",
    "        self.progress_bar = progress_bar\n",
    "\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if not is_llamacpp:\n",
    "            metadata = f'url: https://huggingface.co/{model}\\n' \\\n",
    "                       f'branch: {branch}\\n' \\\n",
    "                       f'download date: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n'\n",
    "\n",
    "            sha256_str = '\\n'.join([f'    {item[1]} {item[0]}' for item in sha256])\n",
    "            if sha256_str:\n",
    "                metadata += f'sha256sum:\\n{sha256_str}'\n",
    "\n",
    "            metadata += '\\n'\n",
    "            (output_folder / 'huggingface-metadata.txt').write_text(metadata)\n",
    "\n",
    "        if specific_file:\n",
    "            print(f\"Downloading {specific_file} to {output_folder}\")\n",
    "        else:\n",
    "            print(f\"Downloading the model to {output_folder}\")\n",
    "\n",
    "        self.start_download_threads(links, output_folder, start_from_scratch=start_from_scratch, threads=threads)\n",
    "\n",
    "    def check_model_files(self, model, branch, links, sha256, output_folder):\n",
    "        validated = True\n",
    "        for i in range(len(sha256)):\n",
    "            fpath = (output_folder / sha256[i][0])\n",
    "\n",
    "            if not fpath.exists():\n",
    "                print(f\"The following file is missing: {fpath}\")\n",
    "                validated = False\n",
    "                continue\n",
    "\n",
    "            with open(output_folder / sha256[i][0], \"rb\") as f:\n",
    "                bytes = f.read()\n",
    "                file_hash = hashlib.sha256(bytes).hexdigest()\n",
    "                if file_hash != sha256[i][1]:\n",
    "                    print(f'Checksum failed: {sha256[i][0]}  {sha256[i][1]}')\n",
    "                    validated = False\n",
    "                else:\n",
    "                    print(f'Checksum validated: {sha256[i][0]}  {sha256[i][1]}')\n",
    "\n",
    "        if validated:\n",
    "            print('[+] Validated checksums of all model files!')\n",
    "        else:\n",
    "            print('[-] Invalid checksums. Rerun download-model.py with the --clean flag.')\n",
    "\n",
    "# Cell 3: Define input parameters and run the downloader\n",
    "\n",
    "# Instead of argparse, directly define the arguments here\n",
    "model = 'meta-llama/Meta-Llama-3.1-8B'  # Example: You can change this\n",
    "branch = 'main'\n",
    "threads = 4\n",
    "text_only = False\n",
    "specific_file = None\n",
    "output = None\n",
    "model_dir = None\n",
    "clean = False\n",
    "check = False\n",
    "max_retries = 5\n",
    "\n",
    "# Initialize the downloader\n",
    "downloader = ModelDownloader(max_retries=max_retries)\n",
    "\n",
    "# Clean up the model/branch names\n",
    "model, branch = downloader.sanitize_model_and_branch_names(model, branch)\n",
    "\n",
    "# Get the download links from Hugging Face\n",
    "links, sha256, is_lora, is_llamacpp = downloader.get_download_links_from_huggingface(model, branch, text_only=text_only, specific_file=specific_file)\n",
    "\n",
    "# Get the output folder\n",
    "output_folder = downloader.get_output_folder(model, branch, is_lora, is_llamacpp=is_llamacpp, model_dir=model_dir)\n",
    "\n",
    "if check:\n",
    "    # Check previously downloaded files\n",
    "    downloader.check_model_files(model, branch, links, sha256, output_folder)\n",
    "else:\n",
    "    # Download files\n",
    "    downloader.download_model_files(model, branch, links, sha256, output_folder, threads=threads, specific_file=specific_file, is_llamacpp=is_llamacpp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "32000\n",
      "LlamaTokenizer (LLaMA 2 or earlier):\n",
      "Tokens: <s>This is a test sentence for tokenization comparison. How do you like it?\n",
      "Token IDs: [1, 851, 349, 264, 1369, 12271, 354, 6029, 1837, 10367, 28723, 1602, 511, 368, 737, 378, 28804]\n",
      "\n",
      "AutoTokenizer (LLaMA 3):\n",
      "Tokens: [1, 1, 28705, 851, 349, 264, 1369, 12271, 354, 6029, 1837, 10367, 28723, 1602, 511, 368, 737, 378, 28804]\n",
      "Token IDs: [1, 851, 349, 264, 1369, 12271, 354, 6029, 1837, 10367, 28723, 1602, 511, 368, 737, 378, 28804]\n",
      "33 tokens start with 'X' in LlamaTokenizer (LLaMA 2)\n",
      "33 tokens start with 'X' in AutoTokenizer (LLaMA 3)\n",
      "33 tokens start with 'Q' in LlamaTokenizer (LLaMA 2)\n",
      "33 tokens start with 'Q' in AutoTokenizer (LLaMA 3)\n",
      "32000\n",
      "32000\n",
      "The lists are not identical\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[18674, 26125, 10886, 19441, 21894, 22135, 11377, 5580, 2682, 10510]\n",
      "[31990, 31991, 31992, 31993, 31994, 31995, 31996, 31997, 31998, 31999]\n",
      "[15297, 14026, 30131, 26791, 31362, 4817, 23476, 21378, 23568, 2371]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, AutoTokenizer, LlamaTokenizerFast, PreTrainedTokenizerFast\n",
    "import os\n",
    "os.chdir(\"/home/feline/master-generation\")\n",
    "\n",
    "def count_tokens_starting_with_Q(tokenizer, tokenizer_name):\n",
    "    count = 0\n",
    "    \n",
    "    # Loop over the tokenizer's vocabulary\n",
    "    for token_id in range(tokenizer.vocab_size):\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        \n",
    "        # Check if the token starts with 'X'\n",
    "        if token.startswith('Q') or token.startswith('ĠQ') or token.startswith('▁Q'):\n",
    "            count += 1\n",
    "\n",
    "    print(f\"{count} tokens start with 'X' in {tokenizer_name}\")\n",
    "    return count\n",
    "\n",
    "import re\n",
    "from transformers import LlamaTokenizer, AutoTokenizer\n",
    "\n",
    "def count_tokens_starting_with_X(tokenizer, tokenizer_name):\n",
    "    count = 0\n",
    "    \n",
    "    # Define a regex to remove any non-alphabetic characters from the start of the token\n",
    "    non_alpha_prefix = re.compile(r'^[^a-zA-Z]+')\n",
    "\n",
    "    # Loop over the tokenizer's vocabulary\n",
    "    for token_id in range(tokenizer.vocab_size):\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        \n",
    "        # Remove any non-alphabetic characters at the start of the token\n",
    "        stripped_token = non_alpha_prefix.sub('', token)\n",
    "        \n",
    "        # Check if the cleaned token starts with 'X'\n",
    "        if stripped_token.startswith('Q'):\n",
    "            count += 1\n",
    "\n",
    "    print(f\"{count} tokens start with 'Q' in {tokenizer_name}\")\n",
    "    return count\n",
    "\n",
    "# Define your text input\n",
    "text_input = \"This is a test sentence for tokenization comparison. How do you like it?\"\n",
    "\n",
    "# Load LlamaTokenizer (for LLaMA 2 or earlier)\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained('models/dolphin-2.6-mistral-7b-Mistral-7B-Instruct-v0.1')\n",
    "\n",
    "\n",
    "# Load AutoTokenizer (for LLaMA 3)\n",
    "#auto_tokenizer = PreTrainedTokenizerFast.from_pretrained('models/Meta-Llama-3.1-8B')\n",
    "auto_tokenizer = LlamaTokenizerFast.from_pretrained('models/dolphin-2.6-mistral-7b-Mistral-7B-Instruct-v0.1')\n",
    "auto_tokenizer.encode_special_tokens = True\n",
    "\n",
    "print(len(llama_tokenizer))\n",
    "print(len(auto_tokenizer))\n",
    "\n",
    "\n",
    "# Tokenize the input using LlamaTokenizer\n",
    "llama_tokens = llama_tokenizer.tokenize(text_input)\n",
    "llama_token_ids = llama_tokenizer.encode(text_input)\n",
    "\n",
    "# Tokenize the input using AutoTokenizer\n",
    "auto_tokens = auto_tokenizer.tokenize(text_input)\n",
    "auto_token_ids = auto_tokenizer.encode(text_input)\n",
    "\n",
    "# Print results for comparison\n",
    "print(\"LlamaTokenizer (LLaMA 2 or earlier):\")\n",
    "print(f\"Tokens: {llama_tokenizer.decode(llama_token_ids)}\")\n",
    "print(f\"Token IDs: {llama_token_ids}\")\n",
    "\n",
    "print(\"\\nAutoTokenizer (LLaMA 3):\")\n",
    "print(f\"Tokens: {auto_tokenizer.encode(auto_tokenizer.decode(auto_token_ids))}\")\n",
    "print(f\"Token IDs: {auto_token_ids}\")\n",
    "\n",
    "\n",
    "# Count tokens that start with \"Q\" in both tokenizers\n",
    "count_tokens_starting_with_Q(llama_tokenizer, 'LlamaTokenizer (LLaMA 2)')\n",
    "count_tokens_starting_with_Q(auto_tokenizer, 'AutoTokenizer (LLaMA 3)')\n",
    "\n",
    "# Count tokens that start with \"X\" in both tokenizers\n",
    "count_tokens_starting_with_X(llama_tokenizer, 'LlamaTokenizer (LLaMA 2)')\n",
    "count_tokens_starting_with_X(auto_tokenizer, 'AutoTokenizer (LLaMA 3)')\n",
    "\n",
    "\n",
    "llama_tokenizer.add_bos_token = False\n",
    "auto_tokenizer.add_bos_token = False\n",
    "vocab=list(llama_tokenizer.get_vocab().values())\n",
    "vocabfast=list(auto_tokenizer.get_vocab().values())\n",
    "print(len(vocab))\n",
    "print(len(vocabfast))\n",
    "if vocab == vocabfast:\n",
    "    print(\"The lists are identical\")\n",
    "else:\n",
    "    print(\"The lists are not identical\")\n",
    "\n",
    "print(vocab[:10])\n",
    "print(vocabfast[:10])\n",
    "\n",
    "print(vocab[-10:])\n",
    "print(vocabfast[-10:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
