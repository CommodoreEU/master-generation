{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autoawq\n",
      "  Downloading autoawq-0.2.6-cp39-cp39-manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting torch==2.3.1 (from autoawq)\n",
      "  Downloading torch-2.3.1-cp39-cp39-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: transformers>=4.35.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from autoawq) (4.44.2)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from autoawq) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from autoawq) (4.11.0)\n",
      "Requirement already satisfied: accelerate in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from autoawq) (0.34.0)\n",
      "Requirement already satisfied: datasets in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from autoawq) (2.19.1)\n",
      "Collecting zstandard (from autoawq)\n",
      "  Downloading zstandard-0.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting autoawq-kernels (from autoawq)\n",
      "  Downloading autoawq_kernels-0.0.8-cp39-cp39-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: filelock in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from torch==2.3.1->autoawq) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from torch==2.3.1->autoawq) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from torch==2.3.1->autoawq) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from torch==2.3.1->autoawq) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from torch==2.3.1->autoawq) (2024.3.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.1 (from torch==2.3.1->autoawq)\n",
      "  Downloading triton-2.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from tokenizers>=0.12.1->autoawq) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from transformers>=4.35.0->autoawq) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from transformers>=4.35.0->autoawq) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from transformers>=4.35.0->autoawq) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from transformers>=4.35.0->autoawq) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from transformers>=4.35.0->autoawq) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from transformers>=4.35.0->autoawq) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from transformers>=4.35.0->autoawq) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from accelerate->autoawq) (5.9.0)\n",
      "INFO: pip is looking at multiple versions of autoawq-kernels to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting autoawq-kernels (from autoawq)\n",
      "  Downloading autoawq_kernels-0.0.7-cp39-cp39-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from datasets->autoawq) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from datasets->autoawq) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from datasets->autoawq) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from datasets->autoawq) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from datasets->autoawq) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from datasets->autoawq) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from aiohttp->datasets->autoawq) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from aiohttp->datasets->autoawq) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from aiohttp->datasets->autoawq) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from aiohttp->datasets->autoawq) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from aiohttp->datasets->autoawq) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from aiohttp->datasets->autoawq) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from requests->transformers>=4.35.0->autoawq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from requests->transformers>=4.35.0->autoawq) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from requests->transformers>=4.35.0->autoawq) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from requests->transformers>=4.35.0->autoawq) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from jinja2->torch==2.3.1->autoawq) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from pandas->datasets->autoawq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from pandas->datasets->autoawq) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from pandas->datasets->autoawq) (2023.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from sympy->torch==2.3.1->autoawq) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets->autoawq) (1.16.0)\n",
      "Downloading autoawq-0.2.6-cp39-cp39-manylinux2014_x86_64.whl (94 kB)\n",
      "Downloading torch-2.3.1-cp39-cp39-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m131.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-2.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading autoawq_kernels-0.0.7-cp39-cp39-manylinux2014_x86_64.whl (33.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, autoawq-kernels, autoawq\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages/~riton'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1\n",
      "    Uninstalling torch-2.4.1:\n",
      "      Successfully uninstalled torch-2.4.1\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/feline/miniconda3/envs/textgen/lib/python3.9/site-packages/~orch'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed autoawq-0.2.6 autoawq-kernels-0.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.3.1 triton-2.3.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model to models/meta-llama_Meta-Llama-3.1-8B\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "README.md: 100%|██████████████████████████████████████████████████| 39.9k/39.9k [00:00<00:00, 411kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 1/7. Retry begins in 2 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 2/7. Retry begins in 4 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 3/7. Retry begins in 8 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 4/7. Retry begins in 16 seconds.\n",
      "Error downloading config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n",
      "Error downloading USE_POLICY.md: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/USE_POLICY.md.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n",
      "Error downloading generation_config.json: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/generation_config.json.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n",
      "Error downloading model-00001-of-00004.safetensors: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/model-00001-of-00004.safetensors.\n",
      "That was attempt 5/7. Retry begins in 32 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                   initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/std.py:1169\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[0;32m-> 1169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/_base.py:609\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 321\u001b[0m\n\u001b[1;32m    318\u001b[0m     downloader\u001b[38;5;241m.\u001b[39mcheck_model_files(model, branch, links, sha256, output_folder)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     \u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msha256\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecific_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecific_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_llamacpp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_llamacpp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 264\u001b[0m, in \u001b[0;36mModelDownloader.download_model_files\u001b[0;34m(self, model, branch, links, sha256, output_folder, progress_bar, start_from_scratch, threads, specific_file, is_llamacpp)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading the model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_download_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_from_scratch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_from_scratch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 240\u001b[0m, in \u001b[0;36mModelDownloader.start_download_threads\u001b[0;34m(self, file_list, output_folder, start_from_scratch, threads)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_download_threads\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_list, output_folder, start_from_scratch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m--> 240\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_from_scratch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_from_scratch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tqdm_class(ex\u001b[38;5;241m.\u001b[39mmap(fn, \u001b[38;5;241m*\u001b[39miterables, chunksize\u001b[38;5;241m=\u001b[39mchunksize), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/_base.py:637\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/feline/master-generation\")\n",
    "\n",
    "# Cell 1: Import necessary libraries\n",
    "import base64\n",
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import requests\n",
    "import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import ConnectionError, RequestException, Timeout\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "base = os.environ.get(\"HF_ENDPOINT\") or \"https://huggingface.co\"\n",
    "\n",
    "# Cell 2: Define the ModelDownloader class\n",
    "\n",
    "class ModelDownloader:\n",
    "    def __init__(self, max_retries=5):\n",
    "        self.max_retries = max_retries\n",
    "        self.session = self.get_session()\n",
    "\n",
    "    def get_session(self):\n",
    "        session = requests.Session()\n",
    "        if self.max_retries:\n",
    "            session.mount('https://cdn-lfs.huggingface.co', HTTPAdapter(max_retries=self.max_retries))\n",
    "            session.mount('https://huggingface.co', HTTPAdapter(max_retries=self.max_retries))\n",
    "\n",
    "        if os.getenv('HF_USER') is not None and os.getenv('HF_PASS') is not None:\n",
    "            session.auth = (os.getenv('HF_USER'), os.getenv('HF_PASS'))\n",
    "\n",
    "        try:\n",
    "            from huggingface_hub import get_token\n",
    "            token = get_token()\n",
    "        except ImportError:\n",
    "            token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "        if token is not None:\n",
    "            session.headers = {'authorization': f'Bearer {token}'}\n",
    "\n",
    "        return session\n",
    "\n",
    "    def sanitize_model_and_branch_names(self, model, branch):\n",
    "        if model[-1] == '/':\n",
    "            model = model[:-1]\n",
    "\n",
    "        if model.startswith(base + '/'):\n",
    "            model = model[len(base) + 1:]\n",
    "\n",
    "        model_parts = model.split(\":\")\n",
    "        model = model_parts[0] if len(model_parts) > 0 else model\n",
    "        branch = model_parts[1] if len(model_parts) > 1 else branch\n",
    "\n",
    "        if branch is None:\n",
    "            branch = \"main\"\n",
    "        else:\n",
    "            pattern = re.compile(r\"^[a-zA-Z0-9._-]+$\")\n",
    "            if not pattern.match(branch):\n",
    "                raise ValueError(\n",
    "                    \"Invalid branch name. Only alphanumeric characters, period, underscore and dash are allowed.\")\n",
    "\n",
    "        return model, branch\n",
    "\n",
    "    def get_download_links_from_huggingface(self, model, branch, text_only=False, specific_file=None):\n",
    "        session = self.session\n",
    "        page = f\"/api/models/{model}/tree/{branch}\"\n",
    "        cursor = b\"\"\n",
    "\n",
    "        links = []\n",
    "        sha256 = []\n",
    "        classifications = []\n",
    "        has_pytorch = False\n",
    "        has_pt = False\n",
    "        has_gguf = False\n",
    "        has_safetensors = False\n",
    "        is_lora = False\n",
    "        while True:\n",
    "            url = f\"{base}{page}\" + (f\"?cursor={cursor.decode()}\" if cursor else \"\")\n",
    "            r = session.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            content = r.content\n",
    "\n",
    "            dict = json.loads(content)\n",
    "            if len(dict) == 0:\n",
    "                break\n",
    "\n",
    "            for i in range(len(dict)):\n",
    "                fname = dict[i]['path']\n",
    "                if specific_file not in [None, ''] and fname != specific_file:\n",
    "                    continue\n",
    "\n",
    "                if not is_lora and fname.endswith(('adapter_config.json', 'adapter_model.bin')):\n",
    "                    is_lora = True\n",
    "\n",
    "                is_pytorch = re.match(r\"(pytorch|adapter|gptq)_model.*\\.bin\", fname)\n",
    "                is_safetensors = re.match(r\".*\\.safetensors\", fname)\n",
    "                is_pt = re.match(r\".*\\.pt\", fname)\n",
    "                is_gguf = re.match(r'.*\\.gguf', fname)\n",
    "                is_tiktoken = re.match(r\".*\\.tiktoken\", fname)\n",
    "                is_tokenizer = re.match(r\"(tokenizer|ice|spiece).*\\.model\", fname) or is_tiktoken\n",
    "                is_text = re.match(r\".*\\.(txt|json|py|md)\", fname) or is_tokenizer\n",
    "                if any((is_pytorch, is_safetensors, is_pt, is_gguf, is_tokenizer, is_text)):\n",
    "                    if 'lfs' in dict[i]:\n",
    "                        sha256.append([fname, dict[i]['lfs']['oid']])\n",
    "\n",
    "                    if is_text:\n",
    "                        links.append(f\"{base}/{model}/resolve/{branch}/{fname}\")\n",
    "                        classifications.append('text')\n",
    "                        continue\n",
    "\n",
    "                    if not text_only:\n",
    "                        links.append(f\"{base}/{model}/resolve/{branch}/{fname}\")\n",
    "                        if is_safetensors:\n",
    "                            has_safetensors = True\n",
    "                            classifications.append('safetensors')\n",
    "                        elif is_pytorch:\n",
    "                            has_pytorch = True\n",
    "                            classifications.append('pytorch')\n",
    "                        elif is_pt:\n",
    "                            has_pt = True\n",
    "                            classifications.append('pt')\n",
    "                        elif is_gguf:\n",
    "                            has_gguf = True\n",
    "                            classifications.append('gguf')\n",
    "\n",
    "            cursor = base64.b64encode(f'{{\"file_name\":\"{dict[-1][\"path\"]}\"}}'.encode()) + b':50'\n",
    "            cursor = base64.b64encode(cursor)\n",
    "            cursor = cursor.replace(b'=', b'%3D')\n",
    "\n",
    "        if (has_pytorch or has_pt or has_gguf) and has_safetensors:\n",
    "            has_gguf = False\n",
    "            for i in range(len(classifications) - 1, -1, -1):\n",
    "                if classifications[i] in ['pytorch', 'pt', 'gguf']:\n",
    "                    links.pop(i)\n",
    "\n",
    "        if has_gguf and specific_file is None:\n",
    "            has_q4km = False\n",
    "            for i in range(len(classifications) - 1, -1, -1):\n",
    "                if 'q4_k_m' in links[i].lower():\n",
    "                    has_q4km = True\n",
    "\n",
    "            if has_q4km:\n",
    "                for i in range(len(classifications) - 1, -1, -1):\n",
    "                    if 'q4_k_m' not in links[i].lower():\n",
    "                        links.pop(i)\n",
    "            else:\n",
    "                for i in range(len(classifications) - 1, -1, -1):\n",
    "                    if links[i].lower().endswith('.gguf'):\n",
    "                        links.pop(i)\n",
    "\n",
    "        is_llamacpp = has_gguf and specific_file is not None\n",
    "        return links, sha256, is_lora, is_llamacpp\n",
    "\n",
    "    def get_output_folder(self, model, branch, is_lora, is_llamacpp=False, model_dir=None):\n",
    "        if model_dir:\n",
    "            base_folder = model_dir\n",
    "        else:\n",
    "            base_folder = 'models' if not is_lora else 'loras'\n",
    "\n",
    "        if is_llamacpp:\n",
    "            return Path(base_folder)\n",
    "\n",
    "        output_folder = f\"{'_'.join(model.split('/')[-2:])}\"\n",
    "        if branch != 'main':\n",
    "            output_folder += f'_{branch}'\n",
    "\n",
    "        output_folder = Path(base_folder) / output_folder\n",
    "        return output_folder\n",
    "\n",
    "    def get_single_file(self, url, output_folder, start_from_scratch=False):\n",
    "        filename = Path(url.rsplit('/', 1)[1])\n",
    "        output_path = output_folder / filename\n",
    "\n",
    "        max_retries = 7\n",
    "        attempt = 0\n",
    "        while attempt < max_retries:\n",
    "            attempt += 1\n",
    "            session = self.session\n",
    "            headers = {}\n",
    "            mode = 'wb'\n",
    "\n",
    "            try:\n",
    "                if output_path.exists() and not start_from_scratch:\n",
    "                    r = session.get(url, stream=True, timeout=20)\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    if output_path.stat().st_size >= total_size:\n",
    "                        return\n",
    "\n",
    "                    headers = {'Range': f'bytes={output_path.stat().st_size}-'}\n",
    "                    mode = 'ab'\n",
    "\n",
    "                with session.get(url, stream=True, headers=headers, timeout=30) as r:\n",
    "                    r.raise_for_status()\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    block_size = 1024 * 1024\n",
    "\n",
    "                    filename_str = str(filename)\n",
    "\n",
    "                    tqdm_kwargs = {\n",
    "                        'total': total_size,\n",
    "                        'unit': 'B',\n",
    "                        'unit_scale': True,\n",
    "                        'unit_divisor': 1024,\n",
    "                        'bar_format': '{desc}{percentage:3.0f}%|{bar:50}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n",
    "                        'desc': f\"{filename_str}: \"\n",
    "                    }\n",
    "\n",
    "                    if 'COLAB_GPU' in os.environ:\n",
    "                        tqdm_kwargs.update({\n",
    "                            'position': 0,\n",
    "                            'leave': True\n",
    "                        })\n",
    "\n",
    "                    with open(output_path, mode) as f:\n",
    "                        with tqdm.tqdm(**tqdm_kwargs) as t:\n",
    "                            count = 0\n",
    "                            for data in r.iter_content(block_size):\n",
    "                                f.write(data)\n",
    "                                t.update(len(data))\n",
    "                                if total_size != 0 and self.progress_bar is not None:\n",
    "                                    count += len(data)\n",
    "                                    self.progress_bar(float(count) / float(total_size), f\"{filename_str}\")\n",
    "\n",
    "                    break\n",
    "            except (RequestException, ConnectionError, Timeout) as e:\n",
    "                print(f\"Error downloading {filename}: {e}.\")\n",
    "                print(f\"That was attempt {attempt}/{max_retries}.\", end=' ')\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Retry begins in {2 ** attempt} seconds.\")\n",
    "                    sleep(2 ** attempt)\n",
    "                else:\n",
    "                    print(\"Failed to download after the maximum number of attempts.\")\n",
    "\n",
    "    def start_download_threads(self, file_list, output_folder, start_from_scratch=False, threads=4):\n",
    "        thread_map(lambda url: self.get_single_file(url, output_folder, start_from_scratch=start_from_scratch), file_list, max_workers=threads, disable=True)\n",
    "\n",
    "    def download_model_files(self, model, branch, links, sha256, output_folder, progress_bar=None, start_from_scratch=False, threads=4, specific_file=None, is_llamacpp=False):\n",
    "        self.progress_bar = progress_bar\n",
    "\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if not is_llamacpp:\n",
    "            metadata = f'url: https://huggingface.co/{model}\\n' \\\n",
    "                       f'branch: {branch}\\n' \\\n",
    "                       f'download date: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n'\n",
    "\n",
    "            sha256_str = '\\n'.join([f'    {item[1]} {item[0]}' for item in sha256])\n",
    "            if sha256_str:\n",
    "                metadata += f'sha256sum:\\n{sha256_str}'\n",
    "\n",
    "            metadata += '\\n'\n",
    "            (output_folder / 'huggingface-metadata.txt').write_text(metadata)\n",
    "\n",
    "        if specific_file:\n",
    "            print(f\"Downloading {specific_file} to {output_folder}\")\n",
    "        else:\n",
    "            print(f\"Downloading the model to {output_folder}\")\n",
    "\n",
    "        self.start_download_threads(links, output_folder, start_from_scratch=start_from_scratch, threads=threads)\n",
    "\n",
    "    def check_model_files(self, model, branch, links, sha256, output_folder):\n",
    "        validated = True\n",
    "        for i in range(len(sha256)):\n",
    "            fpath = (output_folder / sha256[i][0])\n",
    "\n",
    "            if not fpath.exists():\n",
    "                print(f\"The following file is missing: {fpath}\")\n",
    "                validated = False\n",
    "                continue\n",
    "\n",
    "            with open(output_folder / sha256[i][0], \"rb\") as f:\n",
    "                bytes = f.read()\n",
    "                file_hash = hashlib.sha256(bytes).hexdigest()\n",
    "                if file_hash != sha256[i][1]:\n",
    "                    print(f'Checksum failed: {sha256[i][0]}  {sha256[i][1]}')\n",
    "                    validated = False\n",
    "                else:\n",
    "                    print(f'Checksum validated: {sha256[i][0]}  {sha256[i][1]}')\n",
    "\n",
    "        if validated:\n",
    "            print('[+] Validated checksums of all model files!')\n",
    "        else:\n",
    "            print('[-] Invalid checksums. Rerun download-model.py with the --clean flag.')\n",
    "\n",
    "# Cell 3: Define input parameters and run the downloader\n",
    "\n",
    "# Instead of argparse, directly define the arguments here\n",
    "model = 'meta-llama/Meta-Llama-3.1-8B'  # Example: You can change this\n",
    "branch = 'main'\n",
    "threads = 4\n",
    "text_only = False\n",
    "specific_file = None\n",
    "output = None\n",
    "model_dir = None\n",
    "clean = False\n",
    "check = False\n",
    "max_retries = 5\n",
    "\n",
    "# Initialize the downloader\n",
    "downloader = ModelDownloader(max_retries=max_retries)\n",
    "\n",
    "# Clean up the model/branch names\n",
    "model, branch = downloader.sanitize_model_and_branch_names(model, branch)\n",
    "\n",
    "# Get the download links from Hugging Face\n",
    "links, sha256, is_lora, is_llamacpp = downloader.get_download_links_from_huggingface(model, branch, text_only=text_only, specific_file=specific_file)\n",
    "\n",
    "# Get the output folder\n",
    "output_folder = downloader.get_output_folder(model, branch, is_lora, is_llamacpp=is_llamacpp, model_dir=model_dir)\n",
    "\n",
    "if check:\n",
    "    # Check previously downloaded files\n",
    "    downloader.check_model_files(model, branch, links, sha256, output_folder)\n",
    "else:\n",
    "    # Download files\n",
    "    downloader.download_model_files(model, branch, links, sha256, output_folder, threads=threads, specific_file=specific_file, is_llamacpp=is_llamacpp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "32000\n",
      "LlamaTokenizer (LLaMA 2 or earlier):\n",
      "Tokens: <s>This is a test sentence for tokenization comparison. How do you like it?\n",
      "Token IDs: [1, 851, 349, 264, 1369, 12271, 354, 6029, 1837, 10367, 28723, 1602, 511, 368, 737, 378, 28804]\n",
      "\n",
      "AutoTokenizer (LLaMA 3):\n",
      "Tokens: [1, 1, 28705, 851, 349, 264, 1369, 12271, 354, 6029, 1837, 10367, 28723, 1602, 511, 368, 737, 378, 28804]\n",
      "Token IDs: [1, 851, 349, 264, 1369, 12271, 354, 6029, 1837, 10367, 28723, 1602, 511, 368, 737, 378, 28804]\n",
      "33 tokens start with 'X' in LlamaTokenizer (LLaMA 2)\n",
      "33 tokens start with 'X' in AutoTokenizer (LLaMA 3)\n",
      "33 tokens start with 'Q' in LlamaTokenizer (LLaMA 2)\n",
      "33 tokens start with 'Q' in AutoTokenizer (LLaMA 3)\n",
      "32000\n",
      "32000\n",
      "The lists are not identical\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[18674, 26125, 10886, 19441, 21894, 22135, 11377, 5580, 2682, 10510]\n",
      "[31990, 31991, 31992, 31993, 31994, 31995, 31996, 31997, 31998, 31999]\n",
      "[15297, 14026, 30131, 26791, 31362, 4817, 23476, 21378, 23568, 2371]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, AutoTokenizer, LlamaTokenizerFast, PreTrainedTokenizerFast\n",
    "import os\n",
    "os.chdir(\"/home/feline/master-generation\")\n",
    "\n",
    "def count_tokens_starting_with_Q(tokenizer, tokenizer_name):\n",
    "    count = 0\n",
    "    \n",
    "    # Loop over the tokenizer's vocabulary\n",
    "    for token_id in range(tokenizer.vocab_size):\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        \n",
    "        # Check if the token starts with 'X'\n",
    "        if token.startswith('Q') or token.startswith('ĠQ') or token.startswith('▁Q'):\n",
    "            count += 1\n",
    "\n",
    "    print(f\"{count} tokens start with 'X' in {tokenizer_name}\")\n",
    "    return count\n",
    "\n",
    "import re\n",
    "from transformers import LlamaTokenizer, AutoTokenizer\n",
    "\n",
    "def count_tokens_starting_with_X(tokenizer, tokenizer_name):\n",
    "    count = 0\n",
    "    \n",
    "    # Define a regex to remove any non-alphabetic characters from the start of the token\n",
    "    non_alpha_prefix = re.compile(r'^[^a-zA-Z]+')\n",
    "\n",
    "    # Loop over the tokenizer's vocabulary\n",
    "    for token_id in range(tokenizer.vocab_size):\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        \n",
    "        # Remove any non-alphabetic characters at the start of the token\n",
    "        stripped_token = non_alpha_prefix.sub('', token)\n",
    "        \n",
    "        # Check if the cleaned token starts with 'X'\n",
    "        if stripped_token.startswith('Q'):\n",
    "            count += 1\n",
    "\n",
    "    print(f\"{count} tokens start with 'Q' in {tokenizer_name}\")\n",
    "    return count\n",
    "\n",
    "# Define your text input\n",
    "text_input = \"This is a test sentence for tokenization comparison. How do you like it?\"\n",
    "\n",
    "# Load LlamaTokenizer (for LLaMA 2 or earlier)\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained('models/dolphin-2.6-mistral-7b-Mistral-7B-Instruct-v0.1')\n",
    "\n",
    "\n",
    "# Load AutoTokenizer (for LLaMA 3)\n",
    "#auto_tokenizer = PreTrainedTokenizerFast.from_pretrained('models/Meta-Llama-3.1-8B')\n",
    "auto_tokenizer = LlamaTokenizerFast.from_pretrained('models/dolphin-2.6-mistral-7b-Mistral-7B-Instruct-v0.1')\n",
    "auto_tokenizer.encode_special_tokens = True\n",
    "\n",
    "print(len(llama_tokenizer))\n",
    "print(len(auto_tokenizer))\n",
    "\n",
    "\n",
    "# Tokenize the input using LlamaTokenizer\n",
    "llama_tokens = llama_tokenizer.tokenize(text_input)\n",
    "llama_token_ids = llama_tokenizer.encode(text_input)\n",
    "\n",
    "# Tokenize the input using AutoTokenizer\n",
    "auto_tokens = auto_tokenizer.tokenize(text_input)\n",
    "auto_token_ids = auto_tokenizer.encode(text_input)\n",
    "\n",
    "# Print results for comparison\n",
    "print(\"LlamaTokenizer (LLaMA 2 or earlier):\")\n",
    "print(f\"Tokens: {llama_tokenizer.decode(llama_token_ids)}\")\n",
    "print(f\"Token IDs: {llama_token_ids}\")\n",
    "\n",
    "print(\"\\nAutoTokenizer (LLaMA 3):\")\n",
    "print(f\"Tokens: {auto_tokenizer.encode(auto_tokenizer.decode(auto_token_ids))}\")\n",
    "print(f\"Token IDs: {auto_token_ids}\")\n",
    "\n",
    "\n",
    "# Count tokens that start with \"Q\" in both tokenizers\n",
    "count_tokens_starting_with_Q(llama_tokenizer, 'LlamaTokenizer (LLaMA 2)')\n",
    "count_tokens_starting_with_Q(auto_tokenizer, 'AutoTokenizer (LLaMA 3)')\n",
    "\n",
    "# Count tokens that start with \"X\" in both tokenizers\n",
    "count_tokens_starting_with_X(llama_tokenizer, 'LlamaTokenizer (LLaMA 2)')\n",
    "count_tokens_starting_with_X(auto_tokenizer, 'AutoTokenizer (LLaMA 3)')\n",
    "\n",
    "\n",
    "llama_tokenizer.add_bos_token = False\n",
    "auto_tokenizer.add_bos_token = False\n",
    "vocab=list(llama_tokenizer.get_vocab().values())\n",
    "vocabfast=list(auto_tokenizer.get_vocab().values())\n",
    "print(len(vocab))\n",
    "print(len(vocabfast))\n",
    "if vocab == vocabfast:\n",
    "    print(\"The lists are identical\")\n",
    "else:\n",
    "    print(\"The lists are not identical\")\n",
    "\n",
    "print(vocab[:10])\n",
    "print(vocabfast[:10])\n",
    "\n",
    "print(vocab[-10:])\n",
    "print(vocabfast[-10:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
