{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading _LLama2-7b-gptq-4bit-32g-actorder_True...\n",
      "Loading _LLama2-7b-gptq-4bit-32g-actorder_True...\n",
      "Auto-assiging --gpu-memory 23 for your GPU to try to prevent out-of-memory errors. You can manually set other values.\n",
      "The AutoGPTQ params are: {'model_basename': 'model', 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': False, 'inject_fused_mlp': False, 'use_safetensors': True, 'trust_remote_code': True, 'max_memory': {0: '23GiB', 'cpu': '64GiB'}, 'quantize_config': None, 'disable_exllama': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 28.70 seconds.\n",
      "\n",
      "Loaded the model in 28.70 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (AutoConfig, AutoModelForCausalLM, AutoTokenizer,\n",
    "                          BitsAndBytesConfig)\n",
    "\n",
    "\n",
    "from modules.GPTQ_loader import load_quantized\n",
    "from modules.text_generation import generate_reply\n",
    "import modules.shared as shared\n",
    "from modules.model import load_model\n",
    "\n",
    "import hashlib\n",
    "import spacy\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "shared.model_name = \"_Mistral-7b-gptq-4bit-32g-actorder_True\"\n",
    "shared.model_name = \"_LLama2-7b-gptq-4bit-32g-actorder_True\"\n",
    "\n",
    "print(f\"Loading {shared.model_name}...\")\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "shared.groupsize = 32\n",
    "shared.wbits = 4\n",
    "shared.model, shared.tokenizer = load_model(shared.model_name, gptq = True)\n",
    "\n",
    "print(f\"Loaded the model in {(time.time()-t0):.2f} seconds.\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Lancaster_sensorimotor_norms_for_39707_words.csv', header=0)  #the header is in the first row\n",
    "\n",
    "shared.sensorimotor = df.set_index('Word').T.to_dict('dict')\n",
    "shared.classes = ['Auditory.mean', 'Gustatory.mean','Haptic.mean','Interoceptive.mean','Olfactory.mean','Visual.mean','Foot_leg.mean','Hand_arm.mean','Head.mean','Mouth.mean','Torso.mean']\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\",device=-1)\n",
    "\n",
    "def secure_hash_to_numbers(input_string, range_list):\n",
    "\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    if (input_string is None):\n",
    "        return [5,5]\n",
    "    doc = nlp(input_string)\n",
    "\n",
    "    core = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    core_str = \" \".join(core)\n",
    "\n",
    "    input_string = core_str\n",
    "\n",
    "    \n",
    "\n",
    "    labels = ['Scientific Concepts',\n",
    "        'Technical Explanations',\n",
    "        'Historical Context',\n",
    "        'Cultural Insights',\n",
    "        'Environmental Issues',\n",
    "        'Health and Medicine',\n",
    "        'Technological Developments',\n",
    "        'Economic Theories',\n",
    "        'Political Analysis',\n",
    "        'Philosophical Concepts',\n",
    "        'Educational Methods',\n",
    "        'Psychological Theories',\n",
    "        'Artistic Movements',\n",
    "        'Literary Analysis',\n",
    "        'Global Events',\n",
    "        'Culinary Traditions',\n",
    "        'Mathematical Concepts',\n",
    "        'Physical Principles',\n",
    "        'Astronomical Discoveries',\n",
    "        'Geographical Information',\n",
    "        'Social Dynamics',\n",
    "        'Legal Interpretations',\n",
    "        'Business Strategies',\n",
    "        'Sports and Fitness',\n",
    "        'Linguistic Features',\n",
    "        'Techniques in Science and Technology']\n",
    "\n",
    "    labels2 = ['Science and Technology',\n",
    "        'Health and Environmental Issues',\n",
    "        'Arts, Culture, and History',\n",
    "        'Economic and Political Analysis',\n",
    "        'Philosophy and Psychology',\n",
    "        'Education and Learning Methods',\n",
    "        'Global and Social Dynamics',\n",
    "        'Legal and Ethical Discussions',\n",
    "        'Business and Management',\n",
    "        'Sports, Fitness, and Recreation',\n",
    "        'Language and Literature']\n",
    "        \n",
    "\n",
    "    results = classifier(input_string, labels)\n",
    "    predicted_label = results['labels'][0]\n",
    "    label_index = labels.index(predicted_label)\n",
    "\n",
    "    results2 = classifier(input_string, labels2)\n",
    "    predicted_label2 = results2['labels'][0]\n",
    "    label_index2 = labels2.index(predicted_label2)\n",
    "   \n",
    "    #print(input_string)\n",
    "    #print([label_index2, label_index])\n",
    "    #print(\"--------------------------\")\n",
    "    return [label_index2, label_index]\n",
    "    \n",
    "    \n",
    "    #return result_numbers\n",
    "\n",
    "def get_last_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    if sentences:\n",
    "        return sentences[-1]\n",
    "    else:\n",
    "        return None  # Return None if there are no sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a watermark?\n",
      "[A watermark is a picture, logo or image embedded into your photo (within the photograph). It is used to identify the author of the photo and protect against plagiarism. It also helps to avoid copyright infringement when using others works (for instance, photos on blogs, articles, presentations, websites) without permission from the owner.]\n",
      "[A watermark is a picture, logo or image embedded into your photo (within the photograph). It is used to identify the author of the photo and protect against plagiarism. But you can put anything you like on your photo: artwork, text message, signature, barcode, graphic design, etc. Many people use funny phrases, quotes or funny images as their watermarks. You can see examples here: http://www.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompts = [\\\n",
    "    #\"How can you get from Graz to Vienna?\"\n",
    "    \"What is a watermark?\"\n",
    "    #\"What is motorsports, is it a real sport?\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "#prompt = \"What is the capital of France?\"\n",
    "    question = f'''<|im_start|>system\n",
    "You are a helpful assistant, who always provide explanation. Don't enumerate anwsers, talk for the user, write links or urls or use numbers.<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start>assistant\n",
    "'''\n",
    "\n",
    "    generate_params = {\n",
    "        'max_new_tokens' : 200,\n",
    "        'add_bos_token' : False,\n",
    "        'truncation_length' : 4096,\n",
    "        'custom_stopping_strings' : [\"### Human:\", \"Human:\", \"user:\", \"Q:\",\"<|im_end|>\",\"<|im_start|>system\"],\n",
    "        'ban_eos_token' : False,\n",
    "        'skip_special_tokens' : False,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.95,\n",
    "        'typical_p': 1,\n",
    "        'repetition_penalty': 1.10,\n",
    "        'encoder_repetition_penalty': 1,\n",
    "        'top_k': 40,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'min_length': 0,\n",
    "        'length_penalty': 1,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'early_stopping': False,\n",
    "        'seed' : 0,\n",
    "    }\n",
    "\n",
    "    #no watermark\n",
    "    shared.delta_char = 0.0\n",
    "    shared.delta_first = 0.0\n",
    "    shared.secret_key = [0,0]\n",
    "    done = False\n",
    "    reply_base = \"\"\n",
    "    reply_current = \"\"\n",
    "    current_question = question\n",
    "    shared.new_sentence = False\n",
    "    shared.delta_senso = 0\n",
    "    shared.start = 0\n",
    "    i = 0\n",
    "    while (done is not True and i < 4):\n",
    "        reply_current, done = generate_reply(current_question, generate_params, eos_token='<|im_end|>')\n",
    "\n",
    "        last_sentence = get_last_sentence(reply_current)\n",
    "        #print(\"------------------found end of sentence, last sentence is:\")\n",
    "        #print(f'''[{reply_current}]''')\n",
    "        shared.secret_key = secure_hash_to_numbers(last_sentence,[(0, 10), (0, 25)])\n",
    "        shared.new_sentence = True\n",
    "        shared.start = 0\n",
    "        reply_base += reply_current\n",
    "        current_question = f'''{current_question}{reply_current}'''\n",
    "        i+=1\n",
    "\n",
    "\n",
    "\n",
    "    #both\n",
    "    shared.delta_char = 2.5#2.5\n",
    "    shared.delta_first = 25.0#50\n",
    "    shared.secret_key = [0,0]\n",
    "    done = False\n",
    "    reply_watermark = \"\"\n",
    "    reply_current = \"\"\n",
    "    current_question = question\n",
    "    shared.new_sentence = False\n",
    "    shared.delta_senso = 0\n",
    "    shared.start = 0\n",
    "    i = 0\n",
    "    while (done is not True and i < 4):\n",
    "        reply_current, done = generate_reply(current_question, generate_params, eos_token='<|im_end|>')\n",
    "\n",
    "        last_sentence = get_last_sentence(reply_current)\n",
    "        #print(\"------------------found end of sentence, last sentence is:\")\n",
    "        #print(f'''[{reply_current}]''')\n",
    "        shared.secret_key = secure_hash_to_numbers(last_sentence,[(0, 10), (0, 25)])\n",
    "        shared.new_sentence = True\n",
    "        shared.start = 0\n",
    "        reply_watermark += reply_current\n",
    "        current_question = f'''{current_question}{reply_current}'''\n",
    "        i+=1\n",
    "\n",
    "\n",
    "\n",
    "    #results.append([prompt,reply_base,reply_watermark])\n",
    "    results.append([prompt,reply_base,reply_watermark])\n",
    "\n",
    "\n",
    "    #reply_watermark = generate_reply(question+reply_watermark, generate_params, eos_token='<|im_end|>')\n",
    "    print(prompt)\n",
    "    print(f'''[{reply_base}]''')\n",
    "    print(f'''[{reply_watermark}]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base\n",
      "You can take the train ( ÖBB ) from Graz to Vienna , it takes 2 hours and 45 minutes and costs around 35 euro one way .\n",
      "////////////////////////////////////////////////////////////////////////////////\n",
      "Foot_leg.mean\n",
      "valid words 14, sentence_value 1.792890282142857\n",
      "T\n",
      "The ÖBB is also offering a discount , if you buy your tickets online in advance .\n",
      "////////////////////////////////////////////////////////////////////////////////\n",
      "Head.mean\n",
      "valid words 13, sentence_value 0.9628076602307694\n",
      "W\n",
      "\n",
      "Watermark\n",
      "You can take the train ( ÖBB ) from Graz to Vienna , it takes 2 hours and 45 minutes and costs around 35 euro one way .\n",
      "////////////////////////////////////////////////////////////////////////////////\n",
      "Foot_leg.mean\n",
      "valid words 14, sentence_value 1.792890282142857\n",
      "T\n",
      "The bus ride is faster ( 1h 45 min ) , but you have to change in Klagenfurt and you pay about 30 euros .\n",
      "////////////////////////////////////////////////////////////////////////////////\n",
      "Foot_leg.mean\n",
      "valid words 15, sentence_value 1.529346896733333\n",
      "T\n",
      "To go by car you need about 2,5 hours driving time , you should be prepared for heavy traffic and toll fees and you have to pay parking at destination .\n",
      "////////////////////////////////////////////////////////////////////////////////\n",
      "Foot_leg.mean\n",
      "valid words 26, sentence_value 1.6774696870384613\n",
      "R\n",
      "Renting a bike is free of charge , you travel 2,5 hours and you use bike path all along .\n",
      "////////////////////////////////////////////////////////////////////////////////\n",
      "Mouth.mean\n",
      "valid words 15, sentence_value 2.2891054433999996\n",
      "R\n",
      "[[[0.014024049285145399, 0.36462528141378037, 0.038461538461538464, 2, 1], [8.180339165740009e-07, 0.014377764117704639, 5.689576695493856e-05, 4, 3]]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import hashlib\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import norm\n",
    "from itertools import permutations\n",
    "from collections import Counter\n",
    "import math\n",
    "import modules.shared as shared\n",
    "\n",
    "from scipy.stats import binom\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        current_sentence.append(token.text)\n",
    "        if token.text in (\".\", \"?\", \"!\"):\n",
    "            sentences.append(\" \".join(current_sentence))\n",
    "            current_sentence = []\n",
    "\n",
    "    # Add the last sentence if not followed by a punctuation mark\n",
    "    if current_sentence:\n",
    "        sentences.append(\" \".join(current_sentence))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def get_words_in_sentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    words = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    return words\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Lancaster_sensorimotor_norms_for_39707_words.csv', header=0)  #the header is in the first row\n",
    "\n",
    "shared.sensorimotor = df.set_index('Word').T.to_dict('dict')\n",
    "shared.classes = ['Auditory.mean', 'Gustatory.mean','Haptic.mean','Interoceptive.mean','Olfactory.mean','Visual.mean','Foot_leg.mean','Hand_arm.mean','Head.mean','Mouth.mean','Torso.mean']\n",
    "\n",
    "mean_value = [1.51,0.32,1.07,1.03,0.39,2.90,0.81,1.45,2.28,1.26,0.82]\n",
    "std_deviation = [0.99,0.70,0.93,0.88,0.62,0.90,0.75,0.91,0.72,0.90,0.67]\n",
    "\n",
    "\n",
    "def calculate_probs(reply):\n",
    "    z_scores = []\n",
    "    probabilities = []\n",
    "    correct_acrosticons = 0\n",
    "    old_class = 0\n",
    "\n",
    "    sentences = split_into_sentences(reply)\n",
    "    for idx, sentence in enumerate(sentences, start=1):\n",
    "        print(sentence)\n",
    "        if idx > 1:\n",
    "            if(chr(ord('A') + generated_numbers[1]) == sentence[:1]):\n",
    "                \n",
    "                correct_acrosticons += 1\n",
    "\n",
    "        range_list = [(0, 10), (0, 25)] \n",
    "        generated_numbers = secure_hash_to_numbers(sentence, range_list)\n",
    "\n",
    "        print(\"////////////////////////////////////////////////////////////////////////////////\")\n",
    "        print(shared.classes[generated_numbers[0]])\n",
    "        \n",
    "\n",
    "\n",
    "        sum_of_word_mean = 0\n",
    "        valid_words = 0\n",
    "\n",
    "        color_text = \"\"\n",
    "\n",
    "        words = get_words_in_sentence(sentence)\n",
    "        for word in words:\n",
    "            if word.upper() in shared.sensorimotor:\n",
    "                valid_words += 1\n",
    "                sum_of_word_mean += shared.sensorimotor[word.upper()][shared.classes[old_class]]\n",
    "                #print(f'''{word} : P={shared.sensorimotor[word.upper()][shared.classes[old_class]]}''')\n",
    "                \n",
    "                #color_text += f'''\\colorword{{{val}}}{{{word}}} '''\n",
    "                if shared.sensorimotor[word.upper()][shared.classes[old_class]] > mean_value[old_class]:\n",
    "                    val = (shared.sensorimotor[word.upper()][shared.classes[old_class]]/5)*100\n",
    "                    color_text += f'''\\colorhighlight{{red}}{{{val}}}{{{word}}}'''\n",
    "                else:\n",
    "                    val = (shared.sensorimotor[word.upper()][shared.classes[old_class]]/5)*100\n",
    "                    color_text += f'''\\colorhighlight{{red}}{{{0}}}{{{word}}}'''\n",
    "            else:\n",
    "                color_text += f'''\\colorhighlight{{red}}{{{0}}}{{{word}}}'''\n",
    "\n",
    "        #print(color_text)\n",
    "        \n",
    "        if(valid_words > 0):\n",
    "            sentence_value = sum_of_word_mean/valid_words\n",
    "            print(f'''valid words {valid_words}, sentence_value {sentence_value}''')\n",
    "\n",
    "            \n",
    "            # Calculate Z-score for the new data point\n",
    "            z_score = (sentence_value - mean_value[old_class]) / std_deviation[old_class]\n",
    "        else:\n",
    "            z_score = 0\n",
    "\n",
    "        print(chr(ord('A') + generated_numbers[1]))\n",
    "        # Calculate the probability using the cumulative distribution function (CDF)\n",
    "        probabilities.append(1-norm.cdf(z_score))\n",
    "        z_scores.append(z_score)\n",
    "\n",
    "        old_class = generated_numbers[0]\n",
    "\n",
    "\n",
    "\n",
    "    def probability_of_acrosticon(num_sentences_starting_with_A, total_sentences, probability_of_A=1/26):\n",
    "\n",
    "        # Calculate the binomial probability\n",
    "        return binom.pmf(num_sentences_starting_with_A, total_sentences, probability_of_A)\n",
    "\n",
    "\n",
    "    acrosticon_restult = probability_of_acrosticon(correct_acrosticons, len(sentences)-1)\n",
    "    #print(f\"The Acrosticon probability is: {acrosticon_restult} for {correct_acrosticons} correct acrosticons in {len(sentences)-1} valid sentences.\")\n",
    "\n",
    "    # Example usage:\n",
    "    stouffer_result = 1-norm.cdf(sum(z_scores)/ math.sqrt(len(probabilities)))\n",
    "    #print(probabilities)\n",
    "    #print(f\"The Stouffer's method combinded z_score is the probability of: {stouffer_result}\")\n",
    "    #print(f\"The total probability is: {stouffer_result*acrosticon_restult}\")\n",
    "\n",
    "    #return stouffer_result*acrosticon_restult, len(sentences), correct_acrosticons\n",
    "    return stouffer_result*acrosticon_restult, stouffer_result, acrosticon_restult, len(sentences), correct_acrosticons\n",
    "\n",
    "scores = []\n",
    "for tuple in results:\n",
    "    print(\"Base\")\n",
    "    score_base, stouffer_result_base, acrosticon_restult_base, len_base, acros_base = calculate_probs(tuple[1])\n",
    "    print()\n",
    "    print(\"Watermark\")\n",
    "    score_watermark, stouffer_result_watermark, acrosticon_restult_watermark, len_watermark, acros_watermark = calculate_probs(tuple[2])\n",
    "\n",
    "    scores.append([[score_base,stouffer_result_base, acrosticon_restult_base,len_base,acros_base],[score_watermark, stouffer_result_watermark, acrosticon_restult_watermark, len_watermark, acros_watermark]])\n",
    "\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
